{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 3034.51it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "test_names, test_texts = [], []\n",
    "test_texts_list = []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/test'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('../input/feedback-prize-2021/test/' + f, 'r', encoding='utf-8').read())\n",
    "for t in test_texts:\n",
    "    test_texts_list.append(t.split())\n",
    "test_text_df = pd.DataFrame({'id': test_names, 'text_list': test_texts_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_datasets = Dataset.from_pandas(test_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Didn't find file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/added_tokens.json. We won't load it.\n",
      "loading file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/vocab.json\n",
      "loading file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/merges.txt\n",
      "loading file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/tokenizer.json\n",
      "loading file None\n",
      "loading file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/special_tokens_map.json\n",
      "loading file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f43e4bfbbcb4fe8970dcaf56cbc891b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../input/longformer-preprocess/best_model/longformer-preprocess_fold'+str(0)+'/', add_prefix_space=True)\n",
    "eval_datasets = {'word_ids':[], 'id':[]}\n",
    "def preparing_test_dataset(examples):\n",
    "    encoding = tokenizer(examples['text_list'], truncation=True, padding=False, max_length = 2048, is_split_into_words=True)\n",
    "    total= len(encoding['input_ids'])\n",
    "    for i in range(total):\n",
    "        word_idx = encoding.word_ids(batch_index=i)\n",
    "        eval_datasets['word_ids'].append(word_idx)\n",
    "        eval_datasets['id'].append(examples['id'][i])\n",
    "    return encoding\n",
    "\n",
    "tokenized_test_datasets = test_datasets.map(preparing_test_dataset, batched=True, remove_columns=test_datasets.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing LongformerForTokenClassification.\n",
      "\n",
      "All the weights of LongformerForTokenClassification were initialized from the model checkpoint at ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold0/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForTokenClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1292 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "loading configuration file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold1/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"../input/longformer-baseline-kfold/best_model/longformer-baseline_fold1/\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold1/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of prediciton (5, 1292, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LongformerForTokenClassification.\n",
      "\n",
      "All the weights of LongformerForTokenClassification were initialized from the model checkpoint at ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold1/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForTokenClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1292 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "loading configuration file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold2/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"../input/longformer-baseline-kfold/best_model/longformer-baseline_fold2/\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold2/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of prediciton (5, 1292, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LongformerForTokenClassification.\n",
      "\n",
      "All the weights of LongformerForTokenClassification were initialized from the model checkpoint at ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold2/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForTokenClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1292 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "loading configuration file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold3/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"../input/longformer-baseline-kfold/best_model/longformer-baseline_fold3/\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold3/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of prediciton (5, 1292, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LongformerForTokenClassification.\n",
      "\n",
      "All the weights of LongformerForTokenClassification were initialized from the model checkpoint at ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold3/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForTokenClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1292 to 1536 to be a multiple of `config.attention_window`: 512\n",
      "loading configuration file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold4/config.json\n",
      "Model config LongformerConfig {\n",
      "  \"_name_or_path\": \"../input/longformer-baseline-kfold/best_model/longformer-baseline_fold4/\",\n",
      "  \"architectures\": [\n",
      "    \"LongformerForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_mode\": \"longformer\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"ignore_attention_mask\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 4098,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold4/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of prediciton (5, 1292, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing LongformerForTokenClassification.\n",
      "\n",
      "All the weights of LongformerForTokenClassification were initialized from the model checkpoint at ../input/longformer-baseline-kfold/best_model/longformer-baseline_fold4/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LongformerForTokenClassification for predictions without further training.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 5\n",
      "  Batch size = 1\n",
      "Input ids are automatically padded from 431 to 512 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Input ids are automatically padded from 792 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 749 to 1024 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 466 to 512 to be a multiple of `config.attention_window`: 512\n",
      "Input ids are automatically padded from 1292 to 1536 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of prediciton (5, 1292, 15)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig, Trainer, DataCollatorForTokenClassification, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "all_predictions = 0\n",
    "training_args = TrainingArguments(per_device_eval_batch_size=1, output_dir = '../input')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "for fold in range(5):\n",
    "    model_path = '../input/longformer-preprocess/best_model/longformer-preprocess_fold'+str(fold)+'/'\n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path, config = config)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset=None,\n",
    "        eval_dataset=tokenized_test_datasets,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    predictions, _, _ = trainer.predict(test_dataset = tokenized_test_datasets)\n",
    "    print(\"shape of prediciton\", predictions.shape)\n",
    "    predictions = predictions.astype(np.float32)\n",
    "    predictions = predictions/5\n",
    "    all_predictions+=predictions\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def label_dict():\n",
    "    train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "    classes = train.discourse_type.unique().tolist()\n",
    "    tags = defaultdict()\n",
    "    for i, c in enumerate(classes):\n",
    "        tags[f'B-{c}'] = i\n",
    "        tags[f'I-{c}'] = i + len(classes)\n",
    "    tags[f'O'] = len(classes) * 2\n",
    "    tags[f'Special'] = -100\n",
    "    l2i = dict(tags)\n",
    "    i2l = defaultdict()\n",
    "    for k, v in l2i.items(): \n",
    "        i2l[v] = k\n",
    "    i2l[-100] = 'Special'\n",
    "    i2l = dict(i2l)\n",
    "    N_LABELS = len(i2l) - 1 # not accounting for -100\n",
    "    return i2l, l2i, N_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_evidence(oof):\n",
    "  if not len(oof):\n",
    "    return oof\n",
    "  \n",
    "  def jn(pst, start, end):\n",
    "    return \" \".join([str(x) for x in pst[start:end]])\n",
    "  \n",
    "  thresh = 1\n",
    "  idu = oof['id'].unique()\n",
    "  eoof = oof[oof['class'] == \"Evidence\"]\n",
    "  neoof = oof[oof['class'] != \"Evidence\"]\n",
    "  eoof.index = eoof[['id', 'class']]\n",
    "  for thresh2 in range(26, 27, 1):\n",
    "    retval = []\n",
    "    for idv in tqdm(idu, desc='link_evidence', leave=False):\n",
    "      for c in ['Evidence']:\n",
    "        q = eoof[(eoof['id'] == idv)]\n",
    "        if len(q) == 0:\n",
    "          continue\n",
    "        pst = []\n",
    "        for r in q.itertuples():\n",
    "          pst = [*pst, -1,  *[int(x) for x in r.predictionstring.split()]]\n",
    "        start = 1\n",
    "        end = 1\n",
    "        for i in range(2, len(pst)):\n",
    "          cur = pst[i]\n",
    "          end = i\n",
    "          if  ((cur == -1) and ((pst[i + 1] > pst[end - 1] + thresh) or (pst[i + 1] - pst[start] > thresh2))):\n",
    "            retval.append((idv, c, jn(pst, start, end)))\n",
    "            start = i + 1\n",
    "        v = (idv, c, jn(pst, start, end + 1))\n",
    "        retval.append(v)\n",
    "    roof = pd.DataFrame(retval, columns=['id', 'class', 'predictionstring'])\n",
    "    roof = roof.merge(neoof, how='outer')\n",
    "    return roof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_label(word_prediction_score):\n",
    "    x = np.sum(word_prediction_score, axis=0)\n",
    "    max_label = np.argmax(x, axis=-1)\n",
    "    return max_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_fb_predictions2(\n",
    "    eval_datasets,\n",
    "    predictions,\n",
    "):\n",
    "    proba_thresh = {\n",
    "        \"Lead\": 0.687,\n",
    "        \"Position\": 0.537,\n",
    "        \"Evidence\": 0.637,\n",
    "        \"Claim\": 0.537,\n",
    "        \"Concluding Statement\": 0.687,\n",
    "        \"Counterclaim\": 0.537,\n",
    "        \"Rebuttal\": 0.537,\n",
    "    }\n",
    "    #discourse length threshold\n",
    "    min_thresh = {\n",
    "        \"Lead\": 9,\n",
    "        \"Position\": 5,\n",
    "        \"Evidence\": 14,\n",
    "        \"Claim\": 3,\n",
    "        \"Concluding Statement\": 11,\n",
    "        \"Counterclaim\": 6,\n",
    "        \"Rebuttal\": 4,\n",
    "    }\n",
    "    print(predictions.shape)\n",
    "    softmax = torch.nn.Softmax(dim=-1)\n",
    "    predictions = torch.tensor(predictions)\n",
    "    pred_score = softmax(predictions)\n",
    "    pred_score = pred_score.numpy()\n",
    "    i2l, _, _ = label_dict()\n",
    "    all_prediction = []\n",
    "    all_pred_score=[]\n",
    "    for k, label_pred_score in tqdm(enumerate(pred_score), desc = \"post-processing\"):\n",
    "      each_prediction = []\n",
    "      each_prediction_score = []\n",
    "      word_ids = eval_datasets['word_ids'][k]\n",
    "      previous_word_idx = -1\n",
    "      word_prediction_score=[]\n",
    "      for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx == None:\n",
    "          continue\n",
    "        \n",
    "        elif word_idx != previous_word_idx:\n",
    "          if len(word_prediction_score)!=0:\n",
    "            # find label which have the most score label following each tokens including in one word\n",
    "            max_label = find_max_label(word_prediction_score)\n",
    "            word_prediction_score = [word_prediction_score[i][max_label] for i in range(len(word_prediction_score))]\n",
    "            each_prediction_score.append(word_prediction_score)\n",
    "            each_prediction.append(i2l[max_label])\n",
    "          \n",
    "          previous_word_idx = word_idx\n",
    "          word_prediction_score=[]\n",
    "          word_prediction_score.append(label_pred_score[idx])\n",
    "        \n",
    "        else:\n",
    "          word_prediction_score.append(label_pred_score[idx])\n",
    "      \n",
    "      max_label = find_max_label(word_prediction_score)\n",
    "      word_prediction_score = [word_prediction_score[i][max_label] for i in range(len(word_prediction_score))]  \n",
    "      each_prediction_score.append(word_prediction_score)\n",
    "      each_prediction.append(i2l[max_label])\n",
    "      \n",
    "      all_prediction.append(each_prediction)\n",
    "      all_pred_score.append(each_prediction_score)\n",
    "    final_pred = []\n",
    "    for i in range(len(eval_datasets['id'])):\n",
    "      idx = eval_datasets['id'][i]\n",
    "      pred = all_prediction[i]\n",
    "      pred_score = all_pred_score[i]\n",
    "      j=0\n",
    "      while j < len(pred):\n",
    "        cls = pred[j]\n",
    "        if cls =='O': \n",
    "            j+=1\n",
    "        else: \n",
    "            cls = cls.replace('B', 'I')\n",
    "        end = j+1\n",
    "        while end < len(pred) and pred[end] == cls:\n",
    "            end +=1\n",
    "        final_pred_score = []\n",
    "        for item in pred_score[j:end]:\n",
    "          final_pred_score.extend(item)\n",
    "        if cls != 'O' and cls!='' and sum(final_pred_score)/len(final_pred_score)>=proba_thresh[cls.replace('I-', '')] and len(final_pred_score)>=min_thresh[cls.replace('I-', '')]:\n",
    "            final_pred.append((idx, cls.replace('I-', ''), ' '.join(map(str, list(range(j, end))))))\n",
    "        j = end\n",
    "    oof = pd.DataFrame(final_pred)\n",
    "    oof.columns = ['id', 'class', 'predictionstring']\n",
    "    \n",
    "    oof = link_evidence(oof)\n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1292, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 77.43it/s]\n",
      "                                                    \r"
     ]
    }
   ],
   "source": [
    "oof = postprocess_fb_predictions2(eval_examples=None, eval_datasets=eval_datasets, predictions=all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
