{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "Read original train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "PUNCTUATION = set(\".,;\")\n",
    "\n",
    "BASE_DIR = \"../input/feedback-prize-2021/\"\n",
    "TRAIN_DIR = BASE_DIR + 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR + \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define correction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> correction #1 : new_start, new_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_positions(examples):\n",
    "    \"\"\"\n",
    "    correction #1 : new_start, new_end\n",
    "    \"\"\"\n",
    "    disc_ids = []\n",
    "    new_starts = []\n",
    "    new_ends = []\n",
    "    new_texts = []\n",
    "    \n",
    "    for id_ in examples[\"id\"]:\n",
    "        \n",
    "        with open(f\"{TRAIN_DIR}/{id_}.txt\") as fp:\n",
    "            file_text = fp.read()\n",
    "\n",
    "        discourse_data = df[df[\"id\"] == id_]\n",
    "\n",
    "        discourse_ids = discourse_data[\"discourse_id\"]\n",
    "        discourse_texts = discourse_data[\"discourse_text\"]\n",
    "        discourse_starts = discourse_data[\"discourse_start\"]\n",
    "        for disc_id, disc_text, disc_start in zip(discourse_ids, discourse_texts, discourse_starts):\n",
    "            disc_text = disc_text.strip()\n",
    "\n",
    "            matches = [x for x in re.finditer(re.escape(disc_text), file_text)]\n",
    "            \n",
    "            # disc_text가 file_text와 겹치는 파트를 iter object로 반환\n",
    "            offset = 0\n",
    "            while len(matches) == 0 and offset < len(disc_text):\n",
    "                # disc_text string 통째로에 대해 match되는 게 없는 경우 들어오게 되는 if문. (discourse_text가 문단인 경우가 이렇게 될 수 있음.)\n",
    "                # 여기로 들어오는 사례가 딱 한번밖에 없음 (ID: F91D7BB4277C)\n",
    "                # 그 외엔 discourse_text가 txt file에 모두 그대로 있음.\n",
    "                chunk = disc_text if offset == 0 else disc_text[:-offset]\n",
    "                matches = [x for x in re.finditer(re.escape(chunk), file_text)]\n",
    "                offset += 5\n",
    "            if offset >= len(disc_text):\n",
    "                # 여기로 들어오는 경우는 없음\n",
    "                print(f\"Could not find substring in {disc_id}\")\n",
    "                print(matches)\n",
    "                continue\n",
    "\n",
    "            # There are some instances when there are multiple matches, \n",
    "            # so we'll take the closest one to the original discourse_start\n",
    "            distances = [abs(disc_start-match.start()) for match in matches]\n",
    "            # print(distances, \" \", id_ , \"\\n\")\n",
    "            idx = matches[np.argmin(distances)].start()                 # 시작점은 txt file index를 기준으로\n",
    "\n",
    "            end_idx = idx + len(disc_text)          # 끝점은 disc_text 길이를 기준으로 맞추기\n",
    "\n",
    "            final_text = file_text[idx:end_idx]\n",
    "            \n",
    "            disc_ids.append(disc_id)\n",
    "            new_starts.append(idx)\n",
    "            new_ends.append(idx + len(final_text))\n",
    "            new_texts.append(final_text)\n",
    "            \n",
    "    return {\n",
    "        \"discourse_id\": disc_ids,\n",
    "        \"new_start\": new_starts,\n",
    "        \"new_end\": new_ends,\n",
    "        \"text_by_new_index\": new_texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  correction #2 : predictionstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_predstr(examples):\n",
    "    \"\"\"\n",
    "    correction #2 : predictionstring\n",
    "    \"\"\"\n",
    "    new_pred_strings = []\n",
    "    discourse_ids = []\n",
    "    \n",
    "    for id_ in examples[\"id\"]:\n",
    "        \n",
    "        \n",
    "        with open(f\"../input/feedback-prize-2021/train/{id_}.txt\") as fp:\n",
    "            file_text = fp.read()\n",
    "\n",
    "        discourse_data = df[df[\"id\"] == id_]\n",
    "        \n",
    "        left_idxs = discourse_data[\"new_start\"]\n",
    "        right_idxs = discourse_data[\"new_end\"]\n",
    "        disc_ids = discourse_data[\"discourse_id\"]\n",
    "        \n",
    "        for left_idx, right_idx, disc_id in zip(left_idxs, right_idxs, disc_ids):\n",
    "            start_word_id = len(file_text[:left_idx].split())\n",
    "            \n",
    "            if left_idx > 0 and file_text[left_idx].split() != [] and file_text[left_idx-1].split() != []:\n",
    "                start_word_id -= 1\n",
    "                \n",
    "            end_word_id = start_word_id + len(file_text[left_idx:right_idx].split())\n",
    "            \n",
    "            new_pred_strings.append(\" \".join(list(map(str, range(start_word_id, end_word_id)))))\n",
    "            discourse_ids.append(disc_id)\n",
    "            \n",
    "            \n",
    "    return {\n",
    "        \"new_predictionstring\": new_pred_strings,\n",
    "        \"discourse_id\": discourse_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"id\": df[\"id\"].unique()})   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction #1 : new_start, new_end\n",
    "results = dataset.map(get_new_positions, batched=True, num_proc=4, remove_columns=[\"id\"])\n",
    "df[\"new_start\"] = results[\"new_start\"]\n",
    "df[\"new_end\"] = results[\"new_end\"]\n",
    "df[\"new_discourse_text\"] = results[\"text_by_new_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correction #2 : predictionstring\n",
    "results = dataset.map(get_new_predstr, batched=True, num_proc=4, remove_columns=[\"id\"])\n",
    "df[\"new_predictionstring\"] = results[\"new_predictionstring\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"corrected_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correctied samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "different_value_mask = df[\"new_predictionstring\"] != df[\"predictionstring\"]\n",
    "\n",
    "for idx, row in df[different_value_mask].sample(n=10, random_state=18).iterrows():\n",
    "    file_text = open(f\"../input/feedback-prize-2021/train/{row.id}.txt\").read()\n",
    "    print(\"Old predictionstring=\", row.predictionstring)\n",
    "    print(\"New predictionstring=\", row.new_predictionstring)\n",
    "    print(\"words using old predictionstring=\", [x for i, x in enumerate(file_text.split()) if i in list(map(int, row.predictionstring.split()))])\n",
    "    print(\"words using new predictionstring=\", [x for i, x in enumerate(file_text.split()) if i in list(map(int, row.new_predictionstring.split()))])\n",
    "    print(\"new discourse text=\", row.new_discourse_text)\n",
    "    print(f\"start_idx/end_idx= {row.new_start}/{row.new_end}\")\n",
    "    print(\"discourse_id=\",row.discourse_id, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
