{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "Read original train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "PUNCTUATION = set(\".,;\")\n",
    "\n",
    "BASE_DIR = \"../input/feedback-prize-2021/\"\n",
    "TRAIN_DIR = BASE_DIR + 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(BASE_DIR + \"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define correction methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> correction #1 : new_start, new_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_positions(examples):\n",
    "    \"\"\"\n",
    "    correction #1 : new_start, new_end\n",
    "    \"\"\"\n",
    "    disc_ids = []\n",
    "    new_starts = []\n",
    "    new_ends = []\n",
    "    new_texts = []\n",
    "    \n",
    "    for id_ in examples[\"id\"]:\n",
    "        \n",
    "        with open(f\"{TRAIN_DIR}/{id_}.txt\") as fp:\n",
    "            file_text = fp.read()\n",
    "\n",
    "        discourse_data = df[df[\"id\"] == id_]\n",
    "\n",
    "        discourse_ids = discourse_data[\"discourse_id\"]\n",
    "        discourse_texts = discourse_data[\"discourse_text\"]\n",
    "        discourse_starts = discourse_data[\"discourse_start\"]\n",
    "        for disc_id, disc_text, disc_start in zip(discourse_ids, discourse_texts, discourse_starts):\n",
    "            disc_text = disc_text.strip()\n",
    "\n",
    "            matches = [x for x in re.finditer(re.escape(disc_text), file_text)]\n",
    "            \n",
    "            # disc_text가 file_text와 겹치는 파트를 iter object로 반환\n",
    "            offset = 0\n",
    "            while len(matches) == 0 and offset < len(disc_text):\n",
    "                # disc_text string 통째로에 대해 match되는 게 없는 경우 들어오게 되는 if문. (discourse_text가 문단인 경우가 이렇게 될 수 있음.)\n",
    "                # 여기로 들어오는 사례가 딱 한번밖에 없음 (ID: F91D7BB4277C)\n",
    "                # 그 외엔 discourse_text가 txt file에 모두 그대로 있음.\n",
    "                chunk = disc_text if offset == 0 else disc_text[:-offset]\n",
    "                matches = [x for x in re.finditer(re.escape(chunk), file_text)]\n",
    "                offset += 5\n",
    "            if offset >= len(disc_text):\n",
    "                # 여기로 들어오는 경우는 없음\n",
    "                print(f\"Could not find substring in {disc_id}\")\n",
    "                print(matches)\n",
    "                continue\n",
    "\n",
    "            # There are some instances when there are multiple matches, \n",
    "            # so we'll take the closest one to the original discourse_start\n",
    "            distances = [abs(disc_start-match.start()) for match in matches]\n",
    "            # print(distances, \" \", id_ , \"\\n\")\n",
    "            idx = matches[np.argmin(distances)].start()                 # 시작점은 txt file index를 기준으로\n",
    "\n",
    "            end_idx = idx + len(disc_text)          # 끝점은 disc_text 길이를 기준으로 맞추기\n",
    "\n",
    "            final_text = file_text[idx:end_idx]\n",
    "            \n",
    "            disc_ids.append(disc_id)\n",
    "            new_starts.append(idx)\n",
    "            new_ends.append(idx + len(final_text))\n",
    "            new_texts.append(final_text)\n",
    "            \n",
    "    return {\n",
    "        \"discourse_id\": disc_ids,\n",
    "        \"new_start\": new_starts,\n",
    "        \"new_end\": new_ends,\n",
    "        \"text_by_new_index\": new_texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  correction #2 : predictionstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_predstr(examples):\n",
    "    \"\"\"\n",
    "    correction #2 : predictionstring\n",
    "    \"\"\"\n",
    "    new_pred_strings = []\n",
    "    discourse_ids = []\n",
    "    \n",
    "    for id_ in examples[\"id\"]:\n",
    "        \n",
    "        \n",
    "        with open(f\"../input/feedback-prize-2021/train/{id_}.txt\") as fp:\n",
    "            file_text = fp.read()\n",
    "\n",
    "        discourse_data = df[df[\"id\"] == id_]\n",
    "        \n",
    "        left_idxs = discourse_data[\"new_start\"]\n",
    "        right_idxs = discourse_data[\"new_end\"]\n",
    "        disc_ids = discourse_data[\"discourse_id\"]\n",
    "        \n",
    "        for left_idx, right_idx, disc_id in zip(left_idxs, right_idxs, disc_ids):\n",
    "            start_word_id = len(file_text[:left_idx].split())\n",
    "            \n",
    "            if left_idx > 0 and file_text[left_idx].split() != [] and file_text[left_idx-1].split() != []:\n",
    "                start_word_id -= 1\n",
    "                \n",
    "            end_word_id = start_word_id + len(file_text[left_idx:right_idx].split())\n",
    "            \n",
    "            new_pred_strings.append(\" \".join(list(map(str, range(start_word_id, end_word_id)))))\n",
    "            discourse_ids.append(disc_id)\n",
    "            \n",
    "            \n",
    "    return {\n",
    "        \"new_predictionstring\": new_pred_strings,\n",
    "        \"discourse_id\": discourse_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"id\": df[\"id\"].unique()})   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      " #0:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "\u001b[A\n",
      "\n",
      " #0:  25%|██▌       | 1/4 [00:07<00:23,  7.68s/ba]\n",
      " #0:  50%|█████     | 2/4 [00:15<00:15,  7.83s/ba]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      " #0:  75%|███████▌  | 3/4 [00:23<00:07,  7.81s/ba]\n",
      "\n",
      " #1: 100%|██████████| 4/4 [00:28<00:00,  7.05s/ba]\n",
      "\n",
      " #2: 100%|██████████| 4/4 [00:29<00:00,  7.26s/ba]\n",
      " #0: 100%|██████████| 4/4 [00:30<00:00,  7.54s/ba]\n",
      "\n",
      "\n",
      " #3: 100%|██████████| 4/4 [00:31<00:00,  7.79s/ba]\n"
     ]
    }
   ],
   "source": [
    "# correction #1 : new_start, new_end\n",
    "results = dataset.map(get_new_positions, batched=True, num_proc=4, remove_columns=[\"id\"])\n",
    "df[\"new_start\"] = results[\"new_start\"]\n",
    "df[\"new_end\"] = results[\"new_end\"]\n",
    "df[\"new_discourse_text\"] = results[\"text_by_new_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " #0:   0%|          | 0/4 [00:00<?, ?ba/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      " #0:  25%|██▌       | 1/4 [00:05<00:17,  5.71s/ba]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      " #0:  50%|█████     | 2/4 [00:11<00:11,  5.68s/ba]\n",
      "\u001b[A\n",
      "\n",
      " #0:  75%|███████▌  | 3/4 [00:17<00:05,  5.67s/ba]\n",
      "\u001b[A\n",
      "\n",
      " #1: 100%|██████████| 4/4 [00:22<00:00,  5.51s/ba]\n",
      " #0: 100%|██████████| 4/4 [00:22<00:00,  5.53s/ba]\n",
      "\n",
      " #2: 100%|██████████| 4/4 [00:22<00:00,  5.53s/ba]\n",
      "\n",
      "\n",
      " #3: 100%|██████████| 4/4 [00:22<00:00,  5.56s/ba]\n"
     ]
    }
   ],
   "source": [
    "# correction #2 : predictionstring\n",
    "results = dataset.map(get_new_predstr, batched=True, num_proc=4, remove_columns=[\"id\"])\n",
    "df[\"new_predictionstring\"] = results[\"new_predictionstring\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"corrected_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correctied samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old predictionstring= 0 1 2 3 4 5 6 7 8 9 10 11 12 13\n",
      "New predictionstring= 0 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "words using old predictionstring= ['Not', 'using', 'cars', 'or', 'not', 'using', 'cars', 'as', 'much', 'can', 'be', 'extremely', 'benefitial', 'to']\n",
      "words using new predictionstring= ['Not', 'using', 'cars', 'or', 'not', 'using', 'cars', 'as', 'much', 'can', 'be', 'extremely', 'benefitial']\n",
      "new discourse text= Not using cars or not using cars as much can be extremely benefitial\n",
      "start_idx/end_idx= 0/68\n",
      "discourse_id= 1622497245577.0 \n",
      "\n",
      "Old predictionstring= 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
      "New predictionstring= 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349\n",
      "words using old predictionstring= ['They', 'have', 'gotten', 'the', 'idea', 'of', 'limiting', 'car', 'usage', 'by', 'ordering', 'people', 'with', 'even-numbered', 'license', 'plates', 'to', 'leave', 'their', 'cars', 'at', 'home', 'or', 'suffer', 'a', '22-euro', 'fine', 'on', 'Mondays.', 'The', 'same', 'would', 'apply', 'to', 'odd-numbered', 'plates', 'the', 'following', 'day.', 'With', 'this', 'new', 'policy,', 'it', 'has', 'shown', 'better', 'air', 'in', 'the', 'city', 'of', 'Paris', 'by', 'Monday.', 'I', 'believe', 'this', 'is', 'a', 'great', 'forced', 'rule.', 'We', 'need', 'to', 'change', 'our', 'ways', 'and', 'work', 'better', 'in', 'making', 'our', 'air', 'cleaner.', 'Traffic']\n",
      "words using new predictionstring= ['better.', 'They', 'have', 'gotten', 'the', 'idea', 'of', 'limiting', 'car', 'usage', 'by', 'ordering', 'people', 'with', 'even-numbered', 'license', 'plates', 'to', 'leave', 'their', 'cars', 'at', 'home', 'or', 'suffer', 'a', '22-euro', 'fine', 'on', 'Mondays.', 'The', 'same', 'would', 'apply', 'to', 'odd-numbered', 'plates', 'the', 'following', 'day.', 'With', 'this', 'new', 'policy,', 'it', 'has', 'shown', 'better', 'air', 'in', 'the', 'city', 'of', 'Paris', 'by', 'Monday.', 'I', 'believe', 'this', 'is', 'a', 'great', 'forced', 'rule.', 'We', 'need', 'to', 'change', 'our', 'ways', 'and', 'work', 'better', 'in', 'making', 'our', 'air', 'cleaner.']\n",
      "new discourse text= . They have gotten the idea of limiting car usage by ordering people with even-numbered license plates to leave their cars at home or suffer a 22-euro fine on Mondays. The same would apply to odd-numbered plates the following day. With this new policy, it has shown better air in the city of Paris by Monday. I believe this is a great forced rule. We need to change our ways and work better in making our air cleaner.\n",
      "start_idx/end_idx= 1487/1904\n",
      "discourse_id= 1622217583888.0 \n",
      "\n",
      "Old predictionstring= 121 122 123 124 125 126 127 128 129 130 131 132 133 134\n",
      "New predictionstring= 120 121 122 123 124 125 126 127 128 129 130 131 132 133\n",
      "words using old predictionstring= ['helping', 'the', 'people,', 'I', 'had', 'the', 'side', 'benefit', 'of', 'seeing', 'Europe', 'and', 'China.\"', 'You']\n",
      "words using new predictionstring= ['said,\"Besides', 'helping', 'the', 'people,', 'I', 'had', 'the', 'side', 'benefit', 'of', 'seeing', 'Europe', 'and', 'China.\"']\n",
      "new discourse text= Besides helping the people, I had the side benefit of seeing Europe and China.\"\n",
      "start_idx/end_idx= 675/754\n",
      "discourse_id= 1620230958867.0 \n",
      "\n",
      "Old predictionstring= 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
      "New predictionstring= 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      "words using old predictionstring= ['travels', 'on', 'earth', 'and', 'beyond', 'should', 'not', 'be', 'limited', 'by', 'dangers', 'and', 'doubts', 'but', 'expanded', 'to', 'meet', 'the', 'very', 'edges', 'of', 'imagination', 'and', 'innovation\"', 'is', 'what', 'was', 'stated', 'in', 'the', 'story', 'saying', 'this', 'is', 'a', 'big', 'project', 'we', 'dont', 'want', 'nor', 'need', 'to', 'miss', 'out', 'on.', 'moral']\n",
      "words using new predictionstring= ['things.\"our', 'travels', 'on', 'earth', 'and', 'beyond', 'should', 'not', 'be', 'limited', 'by', 'dangers', 'and', 'doubts', 'but', 'expanded', 'to', 'meet', 'the', 'very', 'edges', 'of', 'imagination', 'and', 'innovation\"', 'is', 'what', 'was', 'stated', 'in', 'the', 'story', 'saying', 'this', 'is', 'a', 'big', 'project', 'we', 'dont', 'want', 'nor', 'need', 'to', 'miss', 'out', 'on.']\n",
      "new discourse text= \"our travels on earth and beyond should not be limited by dangers and doubts but expanded to meet the very edges of imagination and innovation\" is what was stated in the story saying this is a big project we dont want nor need to miss out on.\n",
      "start_idx/end_idx= 824/1066\n",
      "discourse_id= 1616615202077.0 \n",
      "\n",
      "Old predictionstring= 39 40 41 42 43 44 45 46 47 48\n",
      "New predictionstring= 38 39 40 41 42 43 44 45 46 47\n",
      "words using old predictionstring= ['Seeking', 'multiple', 'opinions', 'can', 'help', 'someone', 'make', 'better', 'choices', 'because']\n",
      "words using new predictionstring= ['person.', 'Seeking', 'multiple', 'opinions', 'can', 'help', 'someone', 'make', 'better', 'choices']\n",
      "new discourse text= . Seeking multiple opinions can help someone make better choices\n",
      "start_idx/end_idx= 216/280\n",
      "discourse_id= 1618260151494.0 \n",
      "\n",
      "Old predictionstring= 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207\n",
      "New predictionstring= 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206\n",
      "words using old predictionstring= ['If', 'people', 'were', 'to', 'ask', 'and', 'receive', 'only', 'one', 'response,', 'the', 'outcome', 'becomes', 'very', 'limited', 'and', 'may', 'not', 'connect', 'with', 'a', 'large', 'audience.', 'In', 'order', 'for', 'it', 'to', 'apply', 'to', 'the', 'audience,', 'everyone', 'would', 'have', 'to', 'have', 'the', 'exact', 'same', 'thought', 'process', 'and', 'opinions', 'as', 'the', 'person', 'the', 'author', 'asked;', 'which', 'is', 'very', 'unlikely.', 'If', 'someone', 'wishes', 'to', 'connect', 'with', 'a', 'variety', 'of', 'people,', 'in', 'this', 'case', 'a', 'larger', 'audience,', 'they', 'must', 'have', 'an', 'understanding', 'of', 'many', 'perspective', 'and', 'points', 'of', 'view.', 'This', 'way', 'the', 'outcome', 'will', 'apply', 'to', 'more', 'people', 'with', 'different', 'thoughts', 'and', 'opinions.', 'When']\n",
      "words using new predictionstring= ['topic.', 'If', 'people', 'were', 'to', 'ask', 'and', 'receive', 'only', 'one', 'response,', 'the', 'outcome', 'becomes', 'very', 'limited', 'and', 'may', 'not', 'connect', 'with', 'a', 'large', 'audience.', 'In', 'order', 'for', 'it', 'to', 'apply', 'to', 'the', 'audience,', 'everyone', 'would', 'have', 'to', 'have', 'the', 'exact', 'same', 'thought', 'process', 'and', 'opinions', 'as', 'the', 'person', 'the', 'author', 'asked;', 'which', 'is', 'very', 'unlikely.', 'If', 'someone', 'wishes', 'to', 'connect', 'with', 'a', 'variety', 'of', 'people,', 'in', 'this', 'case', 'a', 'larger', 'audience,', 'they', 'must', 'have', 'an', 'understanding', 'of', 'many', 'perspective', 'and', 'points', 'of', 'view.', 'This', 'way', 'the', 'outcome', 'will', 'apply', 'to', 'more', 'people', 'with', 'different', 'thoughts', 'and', 'opinions.']\n",
      "new discourse text= . If people were to ask and receive only one response, the outcome becomes very limited and may not connect with a large audience. In order for it to apply to the audience, everyone would have to have the exact same thought process and opinions as the person the author asked; which is very unlikely. If someone wishes to connect with a variety of people, in this case a larger audience, they must have an understanding of many perspective and points of view. This way the outcome will apply to more people with different thoughts and opinions.\n",
      "start_idx/end_idx= 703/1247\n",
      "discourse_id= 1617817560512.0 \n",
      "\n",
      "Old predictionstring= 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64\n",
      "New predictionstring= 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63\n",
      "words using old predictionstring= ['also', 'many', 'students', 'in', 'rural', 'areas', 'in', 'the', 'world', 'still', 'dont', 'have', 'access', 'to', 'wifi/internet', 'sadly.', 'The']\n",
      "words using new predictionstring= ['school,and', 'also', 'many', 'students', 'in', 'rural', 'areas', 'in', 'the', 'world', 'still', 'dont', 'have', 'access', 'to', 'wifi/internet', 'sadly.']\n",
      "new discourse text= and also many students in rural areas in the world still dont have access to wifi/internet sadly.\n",
      "start_idx/end_idx= 257/354\n",
      "discourse_id= 1621434748054.0 \n",
      "\n",
      "Old predictionstring= 82 83 84 85\n",
      "New predictionstring= 78 79 80 81\n",
      "words using old predictionstring= ['the', 'Swing', 'States,fourth', 'the']\n",
      "words using new predictionstring= ['Outcome,second', 'that', \"Everyone's\", 'President,third']\n",
      "new discourse text= second that Everyone's President\n",
      "start_idx/end_idx= 507/539\n",
      "discourse_id= 1621447654992.0 \n",
      "\n",
      "Old predictionstring= 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      "New predictionstring= 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      "words using old predictionstring= ['school', 'cannot', 'teach', 'what', 'sport', 'or', 'club', 'that', 'they', 'are', 'offering', 'better', 'than', 'a', 'facility', 'that', 'specializes', 'in', 'what', 'they', 'are', 'offering,', 'Finally,kids']\n",
      "words using new predictionstring= ['sleep,The', 'school', 'cannot', 'teach', 'what', 'sport', 'or', 'club', 'that', 'they', 'are', 'offering', 'better', 'than', 'a', 'facility', 'that', 'specializes', 'in', 'what', 'they', 'are', 'offering,']\n",
      "new discourse text= The school cannot teach what sport or club that they are offering better than a facility that specializes in what they are offering,\n",
      "start_idx/end_idx= 723/855\n",
      "discourse_id= 1616091352249.0 \n",
      "\n",
      "Old predictionstring= 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      "New predictionstring= 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n",
      "words using old predictionstring= ['scientist', 'seeking', 'to', 'conduct', 'a', 'thorough', 'mission', 'to', 'understand', 'Venus', 'would', 'need', 'to', 'get', 'up', 'close', 'and', 'personal', 'despite', 'the', 'risks.', 'Or', 'maybe', 'we', 'should', 'think', 'of', 'them', 'as', 'challenges\"(paragraph', '6)', '.']\n",
      "words using new predictionstring= ['\"Therefore', 'scientist', 'seeking', 'to', 'conduct', 'a', 'thorough', 'mission', 'to', 'understand', 'Venus', 'would', 'need', 'to', 'get', 'up', 'close', 'and', 'personal', 'despite', 'the', 'risks.', 'Or', 'maybe', 'we', 'should', 'think', 'of', 'them', 'as', 'challenges\"(paragraph', '6)']\n",
      "new discourse text= Therefore scientist seeking to conduct a thorough mission to understand Venus would need to get up close and personal despite the risks. Or maybe we should think of them as challenges\"(paragraph 6)\n",
      "start_idx/end_idx= 1/198\n",
      "discourse_id= 1616624349979.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "different_value_mask = df[\"new_predictionstring\"] != df[\"predictionstring\"]\n",
    "\n",
    "for idx, row in df[different_value_mask].sample(n=10, random_state=18).iterrows():\n",
    "    file_text = open(f\"../input/feedback-prize-2021/train/{row.id}.txt\").read()\n",
    "    print(\"Old predictionstring=\", row.predictionstring)\n",
    "    print(\"New predictionstring=\", row.new_predictionstring)\n",
    "    print(\"words using old predictionstring=\", [x for i, x in enumerate(file_text.split()) if i in list(map(int, row.predictionstring.split()))])\n",
    "    print(\"words using new predictionstring=\", [x for i, x in enumerate(file_text.split()) if i in list(map(int, row.new_predictionstring.split()))])\n",
    "    print(\"new discourse text=\", row.new_discourse_text)\n",
    "    print(f\"start_idx/end_idx= {row.new_start}/{row.new_end}\")\n",
    "    print(\"discourse_id=\",row.discourse_id, \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5aa82812c4681f8310f3f49363d12ac4c7be23abd4617d81f239e4ef56509db9"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
