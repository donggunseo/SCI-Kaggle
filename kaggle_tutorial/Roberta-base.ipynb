{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 2087.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DF920E0A7337</td>\n",
       "      <td>Have you ever asked more than one person for h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0FB0700DAF44</td>\n",
       "      <td>During a group project, have you ever asked a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D46BCB48440A</td>\n",
       "      <td>When people ask for advice,they sometimes talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18409261F5C2</td>\n",
       "      <td>80% of Americans believe seeking multiple opin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D72CB1C11673</td>\n",
       "      <td>Making choices in life can be very difficult. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  DF920E0A7337  Have you ever asked more than one person for h...\n",
       "1  0FB0700DAF44  During a group project, have you ever asked a ...\n",
       "2  D46BCB48440A  When people ask for advice,they sometimes talk...\n",
       "3  18409261F5C2  80% of Americans believe seeking multiple opin...\n",
       "4  D72CB1C11673  Making choices in life can be very difficult. ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_names, test_texts = [], []\n",
    "for f in tqdm(list(os.listdir('./test'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    test_texts.append(open('./test/' + f, 'r').read().replace(u'\\xa0', u' '))\n",
    "test_texts = pd.DataFrame({'id': test_names, 'text': test_texts})\n",
    "# test_texts['text'] = test_texts['text'].apply(lambda x:x.split())\n",
    "test_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [00:01<00:00, 8173.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3321A3E87AD3</td>\n",
       "      <td>I do agree that some students would benefit fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DFEAEC512BAB</td>\n",
       "      <td>Should students design a summer project for sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2E4AFCD3987F</td>\n",
       "      <td>Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EB6C2AF20BFE</td>\n",
       "      <td>People sometimes have a different opinion than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A91A08E523D5</td>\n",
       "      <td>Dear senator,\\n\\nAs you know the Electoral Col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  3321A3E87AD3  I do agree that some students would benefit fr...\n",
       "1  DFEAEC512BAB  Should students design a summer project for sc...\n",
       "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...\n",
       "3  EB6C2AF20BFE  People sometimes have a different opinion than...\n",
       "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('./train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('./train/' + f, 'r').read().replace(u'\\xa0', u' '))\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "# train_texts['text'] = test_texts['text'].apply(lambda x:x.split())\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15594it [01:40, 155.69it/s]\n"
     ]
    }
   ],
   "source": [
    "all_entities = []\n",
    "for i in tqdm(train_text_df.iterrows()):\n",
    "    total = i[1]['text'].split().__len__()\n",
    "    start = -1\n",
    "    entities = []\n",
    "    for j in train_df[train_df['id'] == i[1]['id']].iterrows():\n",
    "        discourse = j[1]['discourse_type']\n",
    "        list_ix = j[1]['predictionstring'].split(' ')\n",
    "#         print(j[1]['predictionstring'],'###' ,len(list_ix))\n",
    "        ent = [f\"I-{discourse}\" for ix in list_ix]\n",
    "        ent[0] = f\"B-{discourse}\"\n",
    "        ds = int(list_ix[0])\n",
    "        de = int(list_ix[-1])\n",
    "        if start < ds-1:\n",
    "            ent_add = ['O' for ix in range(int(ds-1-start))]\n",
    "            ent = ent_add + ent\n",
    "#         print(len(entities))\n",
    "#         print(ent, len(ent))\n",
    "        entities.extend(ent)\n",
    "#         print(len(entities))\n",
    "        start = de\n",
    "    if len(entities) < total:\n",
    "        ent_add = [\"O\" for ix in range(total-len(entities))]\n",
    "        entities += ent_add\n",
    "    else:\n",
    "        entities = entities[:total]\n",
    "#     print(i[1]['id'],'@@@@@@@@' ,i[1]['text'].split(' ').__len__(), len(entities))\n",
    "    all_entities.append(entities)\n",
    "#     if len(all_entities) > 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_df['entities'] = all_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3321A3E87AD3</td>\n",
       "      <td>I do agree that some students would benefit fr...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DFEAEC512BAB</td>\n",
       "      <td>Should students design a summer project for sc...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2E4AFCD3987F</td>\n",
       "      <td>Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...</td>\n",
       "      <td>[O, O, O, O, B-Position, I-Position, I-Positio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EB6C2AF20BFE</td>\n",
       "      <td>People sometimes have a different opinion than...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A91A08E523D5</td>\n",
       "      <td>Dear senator,\\n\\nAs you know the Electoral Col...</td>\n",
       "      <td>[O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  3321A3E87AD3  I do agree that some students would benefit fr...   \n",
       "1  DFEAEC512BAB  Should students design a summer project for sc...   \n",
       "2  2E4AFCD3987F  Dear State Senator\\n\\n,\\n\\nIn the ruels of vot...   \n",
       "3  EB6C2AF20BFE  People sometimes have a different opinion than...   \n",
       "4  A91A08E523D5  Dear senator,\\n\\nAs you know the Electoral Col...   \n",
       "\n",
       "                                            entities  \n",
       "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "1  [O, O, O, O, O, O, O, O, B-Position, I-Positio...  \n",
       "2  [O, O, O, O, B-Position, I-Position, I-Positio...  \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...  \n",
       "4  [O, O, B-Lead, I-Lead, I-Lead, I-Lead, I-Lead,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast, RobertaForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pdb\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'model_name': 'roberta-base',\n",
    "         'max_length': 512,\n",
    "         'train_batch_size':8,\n",
    "         'valid_batch_size':16,\n",
    "         'epochs':3,\n",
    "         'learning_rate':1e-05,\n",
    "         'max_grad_norm':10,\n",
    "         'device': 'cuda' if torch.cuda.is_available() else 'cpu'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels = ['O', 'B-Lead', 'I-Lead', 'B-Position', 'I-Position', 'B-Claim', 'I-Claim', 'B-Counterclaim', 'I-Counterclaim', \n",
    "          'B-Rebuttal', 'I-Rebuttal', 'B-Evidence', 'I-Evidence', 'B-Concluding Statement', 'I-Concluding Statement']\n",
    "\n",
    "labels_to_ids = {v:k for k,v in enumerate(output_labels)}\n",
    "ids_to_labels = {k:v for k,v in enumerate(output_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-Lead': 1,\n",
       " 'I-Lead': 2,\n",
       " 'B-Position': 3,\n",
       " 'I-Position': 4,\n",
       " 'B-Claim': 5,\n",
       " 'I-Claim': 6,\n",
       " 'B-Counterclaim': 7,\n",
       " 'I-Counterclaim': 8,\n",
       " 'B-Rebuttal': 9,\n",
       " 'I-Rebuttal': 10,\n",
       " 'B-Evidence': 11,\n",
       " 'I-Evidence': 12,\n",
       " 'B-Concluding Statement': 13,\n",
       " 'I-Concluding Statement': 14}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "  def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # step 1: get the sentence and word labels \n",
    "        sentence = self.data.text[index]\n",
    "        word_labels = self.data.entities[index]\n",
    "\n",
    "        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n",
    "        # BertTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n",
    "        encoding = self.tokenizer(sentence,\n",
    "#                              is_pretokenized=True, \n",
    "#                                   is_split_into_words=True,\n",
    "                             return_offsets_mapping=True, \n",
    "                             padding='max_length', \n",
    "                             truncation=True, \n",
    "                             max_length=self.max_len)\n",
    "        \n",
    "        # step 3: create token labels only for first word pieces of each tokenized word\n",
    "#         pdb.set_trace()\n",
    "        labels = [labels_to_ids[label] for label in word_labels] \n",
    "        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n",
    "        # create an empty array of -100 of length max_length\n",
    "        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "#         print(len(sentence), len(labels))\n",
    "        # set only labels whose first offset position is 0 and the second is not 0\n",
    "        i = 0\n",
    "        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "#             print(idx)\n",
    "            if mapping[0] != 0 and mapping[0] != encoding['offset_mapping'][idx-1][1]:\n",
    "            # overwrite label\n",
    "#             pdb.set_trace()\n",
    "#             print(mapping)\n",
    "#             print(encoded_labels.shape, len(labels), idx, i)\n",
    "                try:\n",
    "                    encoded_labels[idx] = labels[i]\n",
    "                except:\n",
    "                    pass\n",
    "                i += 1\n",
    "            else:\n",
    "                if idx==1:\n",
    "    #                 print(idx)\n",
    "                    encoded_labels[idx] = labels[i]\n",
    "                    i += 1\n",
    "        # step 4: turn everything into PyTorch tensors\n",
    "        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.as_tensor(encoded_labels)\n",
    "        \n",
    "        return item\n",
    "\n",
    "  def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 478M/478M [00:46<00:00, 10.8MB/s] \n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(config['model_name'])\n",
    "model = RobertaForTokenClassification.from_pretrained(config['model_name'], num_labels=len(output_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15594, 2)\n",
      "TRAIN Dataset: (12475, 2)\n",
      "TEST Dataset: (3119, 2)\n"
     ]
    }
   ],
   "source": [
    "data = train_text_df[['text', 'entities']]\n",
    "train_size = 0.8\n",
    "train_dataset = data.sample(frac=train_size,random_state=200)\n",
    "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = dataset(train_dataset, tokenizer, config['max_length'])\n",
    "testing_set = dataset(test_dataset, tokenizer, config['max_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': config['train_batch_size'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': config['valid_batch_size'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 1,\n",
    "                'pin_memory':True\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts_set = dataset(test_texts, tokenizer, config['max_length'])\n",
    "test_texts_loader = DataLoader(test_texts_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0]) @@@\n",
      "<s>  -100\n",
      "tensor([0, 6]) @@@\n",
      "Phones  O\n",
      "tensor([ 7, 10]) @@@\n",
      "Ġand  O\n",
      "tensor([11, 18]) @@@\n",
      "ĠDriving  O\n",
      "tensor([18, 19]) @@@\n",
      "Ċ  -100\n",
      "tensor([19, 20]) @@@\n",
      "Ċ  -100\n",
      "tensor([20, 25]) @@@\n",
      "Every  -100\n",
      "tensor([26, 30]) @@@\n",
      "Ġyear  B-Lead\n",
      "tensor([31, 35]) @@@\n",
      "Ġmany  I-Lead\n",
      "tensor([36, 42]) @@@\n",
      "Ġpeople  I-Lead\n",
      "tensor([43, 49]) @@@\n",
      "Ġaround  I-Lead\n",
      "tensor([50, 53]) @@@\n",
      "Ġthe  I-Lead\n",
      "tensor([54, 59]) @@@\n",
      "Ġworld  I-Lead\n",
      "tensor([60, 63]) @@@\n",
      "Ġare  I-Lead\n",
      "tensor([64, 69]) @@@\n",
      "Ġdying  I-Lead\n",
      "tensor([70, 77]) @@@\n",
      "Ġbecause  I-Lead\n",
      "tensor([78, 80]) @@@\n",
      "Ġof  I-Lead\n",
      "tensor([81, 85]) @@@\n",
      "Ġcell  I-Lead\n",
      "tensor([86, 91]) @@@\n",
      "Ġphone  I-Lead\n",
      "tensor([92, 95]) @@@\n",
      "Ġuse  I-Lead\n",
      "tensor([ 96, 101]) @@@\n",
      "Ġwhile  I-Lead\n",
      "tensor([102, 109]) @@@\n",
      "Ġdriving  I-Lead\n",
      "tensor([109, 110]) @@@\n",
      ".  -100\n",
      "tensor([111, 118]) @@@\n",
      "ĠInstead  I-Lead\n",
      "tensor([119, 121]) @@@\n",
      "Ġof  I-Lead\n",
      "tensor([122, 129]) @@@\n",
      "Ġkeeping  I-Lead\n",
      "tensor([130, 135]) @@@\n",
      "Ġtheir  I-Lead\n",
      "tensor([136, 140]) @@@\n",
      "Ġeyes  I-Lead\n",
      "tensor([141, 143]) @@@\n",
      "Ġon  I-Lead\n",
      "tensor([144, 147]) @@@\n",
      "Ġthe  I-Lead\n",
      "tensor([148, 152]) @@@\n",
      "Ġroad  I-Lead\n",
      "tensor([152, 153]) @@@\n",
      ",  -100\n",
      "tensor([154, 160]) @@@\n",
      "Ġpeople  I-Lead\n",
      "tensor([161, 165]) @@@\n",
      "Ġkeep  I-Lead\n",
      "tensor([166, 171]) @@@\n",
      "Ġtheir  I-Lead\n",
      "tensor([172, 176]) @@@\n",
      "Ġeyes  I-Lead\n",
      "tensor([177, 179]) @@@\n",
      "Ġon  I-Lead\n",
      "tensor([180, 183]) @@@\n",
      "Ġthe  I-Lead\n",
      "tensor([184, 189]) @@@\n",
      "Ġphone  I-Lead\n",
      "tensor([189, 190]) @@@\n",
      ".  -100\n",
      "tensor([191, 195]) @@@\n",
      "ĠThis  I-Lead\n",
      "tensor([196, 201]) @@@\n",
      "Ġleads  I-Lead\n",
      "tensor([202, 204]) @@@\n",
      "Ġto  I-Lead\n",
      "tensor([205, 212]) @@@\n",
      "Ġcrashes  I-Lead\n",
      "tensor([212, 213]) @@@\n",
      ",  -100\n",
      "tensor([214, 219]) @@@\n",
      "Ġdeath  I-Lead\n",
      "tensor([219, 220]) @@@\n",
      ",  -100\n",
      "tensor([221, 224]) @@@\n",
      "Ġand  I-Lead\n",
      "tensor([225, 233]) @@@\n",
      "Ġinjuries  I-Lead\n",
      "tensor([233, 234]) @@@\n",
      ".  -100\n",
      "tensor([235, 240]) @@@\n",
      "ĠWhile  I-Lead\n",
      "tensor([241, 245]) @@@\n",
      "Ġcell  I-Lead\n",
      "tensor([246, 251]) @@@\n",
      "Ġphone  I-Lead\n",
      "tensor([252, 255]) @@@\n",
      "Ġuse  I-Lead\n",
      "tensor([256, 259]) @@@\n",
      "Ġcan  I-Lead\n",
      "tensor([260, 262]) @@@\n",
      "Ġbe  I-Lead\n",
      "tensor([263, 273]) @@@\n",
      "Ġbeneficial  I-Lead\n",
      "tensor([274, 276]) @@@\n",
      "Ġin  I-Lead\n",
      "tensor([277, 282]) @@@\n",
      "Ġtimes  I-Lead\n",
      "tensor([283, 285]) @@@\n",
      "Ġof  I-Lead\n",
      "tensor([286, 295]) @@@\n",
      "Ġemergency  I-Lead\n",
      "tensor([296, 298]) @@@\n",
      "Ġor  I-Lead\n",
      "tensor([299, 304]) @@@\n",
      "Ġwhile  I-Lead\n",
      "tensor([305, 310]) @@@\n",
      "Ġusing  I-Lead\n",
      "tensor([311, 312]) @@@\n",
      "ĠG  I-Lead\n",
      "tensor([312, 313]) @@@\n",
      ".  -100\n",
      "tensor([313, 314]) @@@\n",
      "P  -100\n",
      "tensor([314, 315]) @@@\n",
      ".  -100\n",
      "tensor([315, 316]) @@@\n",
      "S  -100\n",
      "tensor([316, 317]) @@@\n",
      ",  -100\n",
      "tensor([318, 322]) @@@\n",
      "Ġcell  I-Lead\n",
      "tensor([323, 329]) @@@\n",
      "Ġphones  B-Position\n",
      "tensor([330, 336]) @@@\n",
      "Ġshould  I-Position\n",
      "tensor([337, 340]) @@@\n",
      "Ġnot  I-Position\n",
      "tensor([341, 343]) @@@\n",
      "Ġbe  I-Position\n",
      "tensor([344, 348]) @@@\n",
      "Ġused  I-Position\n",
      "tensor([349, 354]) @@@\n",
      "Ġwhile  I-Position\n",
      "tensor([355, 362]) @@@\n",
      "Ġdriving  I-Position\n",
      "tensor([362, 363]) @@@\n",
      ".  -100\n",
      "tensor([363, 364]) @@@\n",
      "Ċ  -100\n",
      "tensor([364, 365]) @@@\n",
      "Ċ  -100\n",
      "tensor([365, 370]) @@@\n",
      "First  -100\n",
      "tensor([370, 371]) @@@\n",
      ",  -100\n",
      "tensor([372, 377]) @@@\n",
      "Ġusing  I-Position\n",
      "tensor([378, 384]) @@@\n",
      "Ġphones  B-Claim\n",
      "tensor([385, 390]) @@@\n",
      "Ġwhile  I-Claim\n",
      "tensor([391, 398]) @@@\n",
      "Ġdriving  I-Claim\n",
      "tensor([399, 402]) @@@\n",
      "Ġcan  I-Claim\n",
      "tensor([403, 407]) @@@\n",
      "Ġlead  I-Claim\n",
      "tensor([408, 410]) @@@\n",
      "Ġto  I-Claim\n",
      "tensor([411, 419]) @@@\n",
      "Ġcrashing  I-Claim\n",
      "tensor([419, 420]) @@@\n",
      ".  -100\n",
      "tensor([421, 424]) @@@\n",
      "ĠYou  I-Claim\n",
      "tensor([425, 428]) @@@\n",
      "Ġcan  I-Claim\n",
      "tensor([429, 435]) @@@\n",
      "Ġeither  B-Evidence\n",
      "tensor([436, 441]) @@@\n",
      "Ġcrash  I-Evidence\n",
      "tensor([442, 446]) @@@\n",
      "Ġinto  I-Evidence\n",
      "tensor([447, 448]) @@@\n",
      "Ġa  I-Evidence\n",
      "tensor([449, 453]) @@@\n",
      "Ġsign  I-Evidence\n",
      "tensor([454, 456]) @@@\n",
      "Ġor  I-Evidence\n",
      "tensor([457, 461]) @@@\n",
      "Ġinto  I-Evidence\n",
      "tensor([462, 466]) @@@\n",
      "Ġsome  I-Evidence\n",
      "tensor([466, 467]) @@@\n",
      "b  -100\n",
      "tensor([467, 472]) @@@\n",
      "odies  -100\n",
      "tensor([472, 473]) @@@\n",
      "'  -100\n",
      "tensor([474, 477]) @@@\n",
      "Ġcar  I-Evidence\n",
      "tensor([477, 478]) @@@\n",
      ".  -100\n",
      "tensor([479, 483]) @@@\n",
      "ĠThis  I-Evidence\n",
      "tensor([484, 489]) @@@\n",
      "Ġleads  I-Evidence\n",
      "tensor([490, 492]) @@@\n",
      "Ġto  I-Evidence\n",
      "tensor([493, 499]) @@@\n",
      "Ġpaying  I-Evidence\n",
      "tensor([500, 508]) @@@\n",
      "Ġsomebody  I-Evidence\n",
      "tensor([509, 514]) @@@\n",
      "Ġmoney  I-Evidence\n",
      "tensor([515, 517]) @@@\n",
      "Ġto  I-Evidence\n",
      "tensor([518, 521]) @@@\n",
      "Ġfix  I-Evidence\n",
      "tensor([522, 526]) @@@\n",
      "Ġyour  I-Evidence\n",
      "tensor([527, 530]) @@@\n",
      "Ġcar  I-Evidence\n",
      "tensor([531, 533]) @@@\n",
      "Ġor  I-Evidence\n",
      "tensor([534, 540]) @@@\n",
      "Ġpaying  I-Evidence\n",
      "tensor([541, 544]) @@@\n",
      "Ġfor  I-Evidence\n",
      "tensor([545, 552]) @@@\n",
      "Ġsomeone  I-Evidence\n",
      "tensor([553, 557]) @@@\n",
      "Ġelse  I-Evidence\n",
      "tensor([557, 559]) @@@\n",
      "'s  -100\n",
      "tensor([560, 563]) @@@\n",
      "Ġcar  I-Evidence\n",
      "tensor([564, 571]) @@@\n",
      "Ġbecause  I-Evidence\n",
      "tensor([572, 575]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([576, 579]) @@@\n",
      "Ġhit  I-Evidence\n",
      "tensor([580, 584]) @@@\n",
      "Ġthem  I-Evidence\n",
      "tensor([584, 585]) @@@\n",
      ".  -100\n",
      "tensor([586, 590]) @@@\n",
      "ĠAlso  I-Evidence\n",
      "tensor([591, 595]) @@@\n",
      "Ġyour  I-Evidence\n",
      "tensor([596, 603]) @@@\n",
      "Ġlicense  I-Evidence\n",
      "tensor([604, 607]) @@@\n",
      "Ġcan  I-Evidence\n",
      "tensor([608, 611]) @@@\n",
      "Ġget  I-Evidence\n",
      "tensor([612, 621]) @@@\n",
      "Ġsuspended  I-Evidence\n",
      "tensor([621, 622]) @@@\n",
      ".  -100\n",
      "tensor([623, 627]) @@@\n",
      "ĠThis  I-Evidence\n",
      "tensor([628, 631]) @@@\n",
      "Ġall  I-Evidence\n",
      "tensor([632, 640]) @@@\n",
      "Ġhappened  I-Evidence\n",
      "tensor([641, 648]) @@@\n",
      "Ġbecause  I-Evidence\n",
      "tensor([649, 652]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([653, 657]) @@@\n",
      "Ġwere  I-Evidence\n",
      "tensor([658, 660]) @@@\n",
      "Ġon  I-Evidence\n",
      "tensor([661, 665]) @@@\n",
      "Ġyour  I-Evidence\n",
      "tensor([666, 671]) @@@\n",
      "Ġphone  I-Evidence\n",
      "tensor([672, 675]) @@@\n",
      "Ġnot  I-Evidence\n",
      "tensor([676, 682]) @@@\n",
      "Ġpaying  I-Evidence\n",
      "tensor([683, 692]) @@@\n",
      "Ġattention  I-Evidence\n",
      "tensor([693, 695]) @@@\n",
      "Ġto  I-Evidence\n",
      "tensor([696, 699]) @@@\n",
      "Ġthe  I-Evidence\n",
      "tensor([700, 704]) @@@\n",
      "Ġroad  I-Evidence\n",
      "tensor([704, 705]) @@@\n",
      ".  -100\n",
      "tensor([705, 706]) @@@\n",
      "Ċ  -100\n",
      "tensor([706, 707]) @@@\n",
      "Ċ  -100\n",
      "tensor([707, 711]) @@@\n",
      "Next  -100\n",
      "tensor([711, 712]) @@@\n",
      ",  -100\n",
      "tensor([713, 726]) @@@\n",
      "Ġunfortunately  I-Evidence\n",
      "tensor([727, 733]) @@@\n",
      "Ġpeople  I-Evidence\n",
      "tensor([734, 737]) @@@\n",
      "Ġwho  B-Claim\n",
      "tensor([738, 741]) @@@\n",
      "Ġget  I-Claim\n",
      "tensor([742, 746]) @@@\n",
      "Ġinto  I-Claim\n",
      "tensor([747, 754]) @@@\n",
      "Ġcrashes  I-Claim\n",
      "tensor([755, 758]) @@@\n",
      "Ġdon  I-Claim\n",
      "tensor([758, 760]) @@@\n",
      "'t  -100\n",
      "tensor([761, 767]) @@@\n",
      "Ġalways  I-Claim\n",
      "tensor([768, 772]) @@@\n",
      "Ġmake  I-Claim\n",
      "tensor([773, 775]) @@@\n",
      "Ġit  I-Claim\n",
      "tensor([775, 776]) @@@\n",
      ".  -100\n",
      "tensor([777, 781]) @@@\n",
      "ĠWhen  I-Claim\n",
      "tensor([782, 785]) @@@\n",
      "Ġyou  I-Claim\n",
      "tensor([785, 788]) @@@\n",
      "'re  -100\n",
      "tensor([789, 791]) @@@\n",
      "Ġon  I-Claim\n",
      "tensor([792, 796]) @@@\n",
      "Ġyour  B-Evidence\n",
      "tensor([797, 802]) @@@\n",
      "Ġphone  I-Evidence\n",
      "tensor([803, 808]) @@@\n",
      "Ġwhile  I-Evidence\n",
      "tensor([809, 812]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([812, 815]) @@@\n",
      "'re  -100\n",
      "tensor([816, 823]) @@@\n",
      "Ġdriving  I-Evidence\n",
      "tensor([824, 827]) @@@\n",
      "Ġand  I-Evidence\n",
      "tensor([828, 831]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([832, 835]) @@@\n",
      "Ġget  I-Evidence\n",
      "tensor([836, 840]) @@@\n",
      "Ġinto  I-Evidence\n",
      "tensor([841, 842]) @@@\n",
      "Ġa  I-Evidence\n",
      "tensor([843, 848]) @@@\n",
      "Ġcrash  I-Evidence\n",
      "tensor([849, 852]) @@@\n",
      "Ġnot  I-Evidence\n",
      "tensor([853, 857]) @@@\n",
      "Ġonly  I-Evidence\n",
      "tensor([858, 861]) @@@\n",
      "Ġcan  I-Evidence\n",
      "tensor([862, 865]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([866, 869]) @@@\n",
      "Ġdie  I-Evidence\n",
      "tensor([870, 873]) @@@\n",
      "Ġbut  I-Evidence\n",
      "tensor([874, 877]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([878, 881]) @@@\n",
      "Ġcan  I-Evidence\n",
      "tensor([882, 886]) @@@\n",
      "Ġkill  I-Evidence\n",
      "tensor([887, 895]) @@@\n",
      "Ġsomebody  I-Evidence\n",
      "tensor([896, 900]) @@@\n",
      "Ġelse  I-Evidence\n",
      "tensor([900, 901]) @@@\n",
      ".  -100\n",
      "tensor([902, 911]) @@@\n",
      "ĠBasically  I-Evidence\n",
      "tensor([911, 912]) @@@\n",
      ",  -100\n",
      "tensor([913, 920]) @@@\n",
      "Ġbecause  I-Evidence\n",
      "tensor([921, 925]) @@@\n",
      "Ġyour  I-Evidence\n",
      "tensor([926, 931]) @@@\n",
      "Ġbeing  I-Evidence\n",
      "tensor([932, 945]) @@@\n",
      "Ġirresponsible  I-Evidence\n",
      "tensor([946, 949]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([950, 953]) @@@\n",
      "Ġcan  I-Evidence\n",
      "tensor([954, 959]) @@@\n",
      "Ġcause  I-Evidence\n",
      "tensor([960, 968]) @@@\n",
      "Ġsomebody  I-Evidence\n",
      "tensor([969, 971]) @@@\n",
      "Ġto  I-Evidence\n",
      "tensor([972, 975]) @@@\n",
      "Ġdie  I-Evidence\n",
      "tensor([975, 976]) @@@\n",
      ".  -100\n",
      "tensor([977, 979]) @@@\n",
      "ĠIs  I-Evidence\n",
      "tensor([980, 982]) @@@\n",
      "Ġit  I-Evidence\n",
      "tensor([983, 989]) @@@\n",
      "Ġreally  I-Evidence\n",
      "tensor([990, 995]) @@@\n",
      "Ġworth  I-Evidence\n",
      "tensor([ 996, 1003]) @@@\n",
      "Ġkilling  I-Evidence\n",
      "tensor([1004, 1012]) @@@\n",
      "Ġsomebody  I-Evidence\n",
      "tensor([1013, 1020]) @@@\n",
      "Ġbecause  I-Evidence\n",
      "tensor([1021, 1024]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([1025, 1029]) @@@\n",
      "Ġwant  I-Evidence\n",
      "tensor([1030, 1032]) @@@\n",
      "Ġto  I-Evidence\n",
      "tensor([1033, 1036]) @@@\n",
      "Ġuse  I-Evidence\n",
      "tensor([1037, 1041]) @@@\n",
      "Ġyour  I-Evidence\n",
      "tensor([1042, 1050]) @@@\n",
      "Ġcellular  I-Evidence\n",
      "tensor([1051, 1057]) @@@\n",
      "Ġdevice  I-Evidence\n",
      "tensor([1058, 1063]) @@@\n",
      "Ġwhile  I-Evidence\n",
      "tensor([1064, 1071]) @@@\n",
      "Ġdriving  I-Evidence\n",
      "tensor([1071, 1072]) @@@\n",
      ".  -100\n",
      "tensor([1073, 1077]) @@@\n",
      "ĠThis  I-Evidence\n",
      "tensor([1078, 1080]) @@@\n",
      "Ġis  I-Evidence\n",
      "tensor([1081, 1088]) @@@\n",
      "Ġanother  I-Evidence\n",
      "tensor([1089, 1095]) @@@\n",
      "Ġreason  I-Evidence\n",
      "tensor([1096, 1102]) @@@\n",
      "Ġphones  I-Evidence\n",
      "tensor([1103, 1110]) @@@\n",
      "Ġshouldn  I-Evidence\n",
      "tensor([1110, 1112]) @@@\n",
      "'t  -100\n",
      "tensor([1113, 1115]) @@@\n",
      "Ġbe  I-Evidence\n",
      "tensor([1116, 1120]) @@@\n",
      "Ġused  I-Evidence\n",
      "tensor([1121, 1126]) @@@\n",
      "Ġwhile  I-Evidence\n",
      "tensor([1127, 1134]) @@@\n",
      "Ġdriving  I-Evidence\n",
      "tensor([1134, 1135]) @@@\n",
      ".  -100\n",
      "tensor([1135, 1136]) @@@\n",
      "Ċ  -100\n",
      "tensor([1136, 1137]) @@@\n",
      "Ċ  -100\n",
      "tensor([1137, 1144]) @@@\n",
      "Finally  -100\n",
      "tensor([1144, 1145]) @@@\n",
      ",  -100\n",
      "tensor([1146, 1152]) @@@\n",
      "Ġphones  I-Evidence\n",
      "tensor([1153, 1160]) @@@\n",
      "Ġshouldn  I-Evidence\n",
      "tensor([1160, 1162]) @@@\n",
      "'t  -100\n",
      "tensor([1163, 1165]) @@@\n",
      "Ġbe  I-Evidence\n",
      "tensor([1166, 1170]) @@@\n",
      "Ġused  B-Claim\n",
      "tensor([1171, 1176]) @@@\n",
      "Ġwhile  I-Claim\n",
      "tensor([1177, 1184]) @@@\n",
      "Ġdriving  I-Claim\n",
      "tensor([1185, 1192]) @@@\n",
      "Ġbecause  I-Claim\n",
      "tensor([1193, 1195]) @@@\n",
      "Ġif  I-Claim\n",
      "tensor([1196, 1199]) @@@\n",
      "Ġyou  I-Claim\n",
      "tensor([1200, 1205]) @@@\n",
      "Ġcrash  I-Claim\n",
      "tensor([1206, 1209]) @@@\n",
      "Ġand  I-Claim\n",
      "tensor([1210, 1213]) @@@\n",
      "Ġyou  I-Claim\n",
      "tensor([1214, 1217]) @@@\n",
      "Ġdon  I-Claim\n",
      "tensor([1217, 1219]) @@@\n",
      "'t  -100\n",
      "tensor([1220, 1223]) @@@\n",
      "Ġdie  I-Claim\n",
      "tensor([1224, 1227]) @@@\n",
      "Ġyou  I-Claim\n",
      "tensor([1228, 1233]) @@@\n",
      "Ġcause  I-Claim\n",
      "tensor([1234, 1242]) @@@\n",
      "Ġinjuries  I-Claim\n",
      "tensor([1243, 1246]) @@@\n",
      "Ġfor  I-Claim\n",
      "tensor([1247, 1251]) @@@\n",
      "Ġlife  I-Claim\n",
      "tensor([1252, 1255]) @@@\n",
      "Ġfor  I-Claim\n",
      "tensor([1256, 1260]) @@@\n",
      "Ġyour  I-Claim\n",
      "tensor([1261, 1267]) @@@\n",
      "Ġvictim  I-Claim\n",
      "tensor([1268, 1270]) @@@\n",
      "Ġor  I-Claim\n",
      "tensor([1271, 1274]) @@@\n",
      "Ġyou  I-Claim\n",
      "tensor([1274, 1275]) @@@\n",
      ".  -100\n",
      "tensor([1276, 1284]) @@@\n",
      "ĠAlthough  I-Claim\n",
      "tensor([1284, 1285]) @@@\n",
      ",  -100\n",
      "tensor([1286, 1292]) @@@\n",
      "Ġpeople  I-Claim\n",
      "tensor([1293, 1302]) @@@\n",
      "Ġsometimes  I-Claim\n",
      "tensor([1303, 1306]) @@@\n",
      "Ġuse  I-Claim\n",
      "tensor([1307, 1313]) @@@\n",
      "Ġphones  B-Evidence\n",
      "tensor([1314, 1317]) @@@\n",
      "Ġfor  I-Evidence\n",
      "tensor([1318, 1319]) @@@\n",
      "ĠG  I-Evidence\n",
      "tensor([1319, 1320]) @@@\n",
      ".  -100\n",
      "tensor([1320, 1321]) @@@\n",
      "P  -100\n",
      "tensor([1321, 1322]) @@@\n",
      ".  -100\n",
      "tensor([1322, 1323]) @@@\n",
      "S  -100\n",
      "tensor([1324, 1328]) @@@\n",
      "Ġsome  I-Evidence\n",
      "tensor([1329, 1333]) @@@\n",
      "Ġcars  I-Evidence\n",
      "tensor([1334, 1337]) @@@\n",
      "Ġare  I-Evidence\n",
      "tensor([1338, 1343]) @@@\n",
      "Ġbuilt  I-Evidence\n",
      "tensor([1344, 1347]) @@@\n",
      "Ġfor  I-Evidence\n",
      "tensor([1348, 1352]) @@@\n",
      "Ġthat  I-Evidence\n",
      "tensor([1352, 1353]) @@@\n",
      ".  -100\n",
      "tensor([1354, 1358]) @@@\n",
      "ĠEven  I-Evidence\n",
      "tensor([1359, 1364]) @@@\n",
      "Ġphone  I-Evidence\n",
      "tensor([1365, 1370]) @@@\n",
      "Ġcalls  I-Evidence\n",
      "tensor([1370, 1371]) @@@\n",
      ".  -100\n",
      "tensor([1372, 1374]) @@@\n",
      "ĠIf  I-Evidence\n",
      "tensor([1375, 1378]) @@@\n",
      "Ġyou  I-Evidence\n",
      "tensor([1379, 1383]) @@@\n",
      "Ġwant  I-Evidence\n",
      "tensor([1384, 1388]) @@@\n",
      "Ġcall  I-Evidence\n",
      "tensor([1389, 1397]) @@@\n",
      "Ġsomebody  I-Evidence\n",
      "tensor([1398, 1402]) @@@\n",
      "Ġhook  I-Evidence\n",
      "tensor([1403, 1407]) @@@\n",
      "Ġyour  I-Evidence\n",
      "tensor([1408, 1413]) @@@\n",
      "Ġphone  I-Evidence\n",
      "tensor([1414, 1416]) @@@\n",
      "Ġup  I-Evidence\n",
      "tensor([1417, 1419]) @@@\n",
      "Ġto  I-Evidence\n",
      "tensor([1420, 1429]) @@@\n",
      "ĠBluetooth  I-Evidence\n",
      "tensor([1430, 1436]) @@@\n",
      "Ġbefore  I-Evidence\n",
      "tensor([1437, 1444]) @@@\n",
      "Ġdriving  I-Evidence\n",
      "tensor([1444, 1445]) @@@\n",
      ".  -100\n",
      "tensor([1446, 1450]) @@@\n",
      "ĠThis  I-Evidence\n",
      "tensor([1451, 1453]) @@@\n",
      "Ġis  I-Evidence\n",
      "tensor([1454, 1461]) @@@\n",
      "Ġanother  I-Evidence\n",
      "tensor([1462, 1468]) @@@\n",
      "Ġreason  I-Evidence\n",
      "tensor([1469, 1472]) @@@\n",
      "Ġwhy  I-Evidence\n",
      "tensor([1473, 1479]) @@@\n",
      "Ġphones  I-Evidence\n",
      "tensor([1480, 1487]) @@@\n",
      "Ġshouldn  I-Evidence\n",
      "tensor([1487, 1489]) @@@\n",
      "'t  -100\n",
      "tensor([1490, 1492]) @@@\n",
      "Ġbe  I-Evidence\n",
      "tensor([1493, 1497]) @@@\n",
      "Ġused  I-Evidence\n",
      "tensor([1498, 1503]) @@@\n",
      "Ġwhile  I-Evidence\n",
      "tensor([1504, 1511]) @@@\n",
      "Ġdriving  I-Evidence\n",
      "tensor([1511, 1512]) @@@\n",
      ".  -100\n",
      "tensor([1512, 1513]) @@@\n",
      "Ċ  -100\n",
      "tensor([1513, 1514]) @@@\n",
      "Ċ  -100\n",
      "tensor([1514, 1516]) @@@\n",
      "So  -100\n",
      "tensor([1517, 1519]) @@@\n",
      "Ġin  I-Evidence\n",
      "tensor([1520, 1530]) @@@\n",
      "Ġconclusion  I-Evidence\n",
      "tensor([1530, 1531]) @@@\n",
      ",  -100\n",
      "tensor([1532, 1538]) @@@\n",
      "Ġphones  I-Evidence\n",
      "tensor([1539, 1545]) @@@\n",
      "Ġshould  I-Evidence\n",
      "tensor([1546, 1549]) @@@\n",
      "Ġnot  B-Concluding Statement\n",
      "tensor([1550, 1552]) @@@\n",
      "Ġbe  I-Concluding Statement\n",
      "tensor([1553, 1557]) @@@\n",
      "Ġused  I-Concluding Statement\n",
      "tensor([1558, 1563]) @@@\n",
      "Ġwhile  I-Concluding Statement\n",
      "tensor([1564, 1571]) @@@\n",
      "Ġdriving  I-Concluding Statement\n",
      "tensor([1571, 1572]) @@@\n",
      ".  -100\n",
      "tensor([1573, 1577]) @@@\n",
      "ĠThis  I-Concluding Statement\n",
      "tensor([1578, 1580]) @@@\n",
      "Ġis  I-Concluding Statement\n",
      "tensor([1581, 1588]) @@@\n",
      "Ġbecause  I-Concluding Statement\n",
      "tensor([1589, 1591]) @@@\n",
      "Ġof  I-Concluding Statement\n",
      "tensor([1592, 1599]) @@@\n",
      "Ġcrashes  I-Concluding Statement\n",
      "tensor([1599, 1600]) @@@\n",
      ",  -100\n",
      "tensor([1601, 1609]) @@@\n",
      "Ġinjuries  I-Concluding Statement\n",
      "tensor([1609, 1610]) @@@\n",
      ",  -100\n",
      "tensor([1611, 1614]) @@@\n",
      "Ġand  I-Concluding Statement\n",
      "tensor([1615, 1623]) @@@\n",
      "Ġpossibly  I-Concluding Statement\n",
      "tensor([1624, 1629]) @@@\n",
      "Ġdeath  I-Concluding Statement\n",
      "tensor([1629, 1630]) @@@\n",
      ".  -100\n",
      "tensor([1631, 1635]) @@@\n",
      "ĠThis  I-Concluding Statement\n",
      "tensor([1636, 1638]) @@@\n",
      "Ġis  I-Concluding Statement\n",
      "tensor([1639, 1643]) @@@\n",
      "Ġjust  I-Concluding Statement\n",
      "tensor([1644, 1657]) @@@\n",
      "Ġirresponsible  I-Concluding Statement\n",
      "tensor([1658, 1666]) @@@\n",
      "Ġbehavior  I-Concluding Statement\n",
      "tensor([1667, 1671]) @@@\n",
      "Ġthat  I-Concluding Statement\n",
      "tensor([1671, 1673]) @@@\n",
      "'s  -100\n",
      "tensor([1674, 1681]) @@@\n",
      "Ġleading  I-Concluding Statement\n",
      "tensor([1682, 1684]) @@@\n",
      "Ġto  I-Concluding Statement\n",
      "tensor([1685, 1693]) @@@\n",
      "Ġproblems  I-Concluding Statement\n",
      "tensor([1694, 1697]) @@@\n",
      "Ġfor  I-Concluding Statement\n",
      "tensor([1698, 1704]) @@@\n",
      "Ġpeople  I-Concluding Statement\n",
      "tensor([1704, 1705]) @@@\n",
      ".  -100\n",
      "tensor([1706, 1716]) @@@\n",
      "ĠEspecially  I-Concluding Statement\n",
      "tensor([1717, 1721]) @@@\n",
      "Ġwhen  I-Concluding Statement\n",
      "tensor([1722, 1727]) @@@\n",
      "Ġthere  I-Concluding Statement\n",
      "tensor([1728, 1731]) @@@\n",
      "Ġare  I-Concluding Statement\n",
      "tensor([1732, 1734]) @@@\n",
      "Ġso  I-Concluding Statement\n",
      "tensor([1735, 1739]) @@@\n",
      "Ġmany  I-Concluding Statement\n",
      "tensor([1740, 1744]) @@@\n",
      "Ġways  I-Concluding Statement\n",
      "tensor([1745, 1747]) @@@\n",
      "Ġto  I-Concluding Statement\n",
      "tensor([1748, 1755]) @@@\n",
      "Ġprevent  I-Concluding Statement\n",
      "tensor([1756, 1759]) @@@\n",
      "Ġthe  I-Concluding Statement\n",
      "tensor([1760, 1769]) @@@\n",
      "Ġsituation  I-Concluding Statement\n",
      "tensor([1769, 1770]) @@@\n",
      ".  -100\n",
      "tensor([1771, 1773]) @@@\n",
      "ĠDo  I-Concluding Statement\n",
      "tensor([1774, 1777]) @@@\n",
      "Ġyou  I-Concluding Statement\n",
      "tensor([1778, 1785]) @@@\n",
      "Ġbelieve  I-Concluding Statement\n",
      "tensor([1786, 1792]) @@@\n",
      "Ġphones  I-Concluding Statement\n",
      "tensor([1793, 1799]) @@@\n",
      "Ġshould  I-Concluding Statement\n",
      "tensor([1800, 1802]) @@@\n",
      "Ġbe  I-Concluding Statement\n",
      "tensor([1803, 1807]) @@@\n",
      "Ġused  I-Concluding Statement\n",
      "tensor([1808, 1813]) @@@\n",
      "Ġwhile  I-Concluding Statement\n",
      "tensor([1814, 1821]) @@@\n",
      "Ġdriving  I-Concluding Statement\n",
      "tensor([1821, 1822]) @@@\n",
      "?  -100\n",
      "tensor([1823, 1823]) @@@\n",
      "Ġ  I-Concluding Statement\n",
      "tensor([1824, 1824]) @@@\n",
      "Ġ  I-Concluding Statement\n",
      "tensor([1825, 1825]) @@@\n",
      "Ġ  I-Concluding Statement\n",
      "tensor([1826, 1826]) @@@\n",
      "Ġ  I-Concluding Statement\n",
      "tensor([1827, 1827]) @@@\n",
      "Ġ  I-Concluding Statement\n",
      "tensor([1828, 1828]) @@@\n",
      "Ġ  -100\n",
      "tensor([1829, 1829]) @@@\n",
      "Ġ  -100\n",
      "tensor([1830, 1830]) @@@\n",
      "Ġ  -100\n",
      "tensor([1831, 1831]) @@@\n",
      "Ġ  -100\n",
      "tensor([1832, 1832]) @@@\n",
      "Ġ  -100\n",
      "tensor([1833, 1833]) @@@\n",
      "Ġ  -100\n",
      "tensor([1834, 1834]) @@@\n",
      "Ġ  -100\n",
      "tensor([1835, 1835]) @@@\n",
      "Ġ  -100\n",
      "tensor([1836, 1836]) @@@\n",
      "Ġ  -100\n",
      "tensor([1837, 1837]) @@@\n",
      "Ġ  -100\n",
      "tensor([1838, 1838]) @@@\n",
      "Ġ  -100\n",
      "tensor([1839, 1839]) @@@\n",
      "Ġ  -100\n",
      "tensor([1840, 1840]) @@@\n",
      "Ġ  -100\n",
      "tensor([1841, 1841]) @@@\n",
      "Ġ  -100\n",
      "tensor([1842, 1842]) @@@\n",
      "Ġ  -100\n",
      "tensor([1843, 1843]) @@@\n",
      "Ġ  -100\n",
      "tensor([1844, 1844]) @@@\n",
      "Ġ  -100\n",
      "tensor([1845, 1845]) @@@\n",
      "Ġ  -100\n",
      "tensor([0, 0]) @@@\n",
      "</s>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n",
      "tensor([0, 0]) @@@\n",
      "<pad>  -100\n"
     ]
    }
   ],
   "source": [
    "## -100으로 레이블링 되는 게 좀 이상함\n",
    "## 출력과정에서 -100은 레이블링을 제외해야 하는것으로 기억하는데 이부분에 대한 구현이 이상함\n",
    "i=0\n",
    "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"input_ids\"]), training_set[0][\"labels\"]):\n",
    "    print(training_set[0]['offset_mapping'][i], '@@@')\n",
    "    if label == -100:\n",
    "        print('{}  {}'.format(token, label))\n",
    "    else:\n",
    "        print('{}  {}'.format(token, ids_to_labels[int(label)]))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5425, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "inputs = training_set[0]\n",
    "input_ids = inputs[\"input_ids\"].unsqueeze(0)\n",
    "attention_mask = inputs[\"attention_mask\"].unsqueeze(0)\n",
    "labels = inputs[\"labels\"].unsqueeze(0)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask, labels=labels, return_dict=False)\n",
    "initial_loss = outputs[0]\n",
    "pred = outputs[1][0]\n",
    "_, pred = torch.max(pred,1)\n",
    "initial_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=config['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        labels = batch['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        loss, tr_logits = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=False)\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 100==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
    "        \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        #-100을 거르는 용도!\n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=config['max_grad_norm']\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(config['epochs']):\n",
    "    print(f\"Training epoch: {epoch + 1}\")\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "            labels = batch['labels'].to(device, dtype = torch.long)\n",
    "            \n",
    "            loss, eval_logits = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=False)\n",
    "            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [ids_to_labels[id.item()] for id in eval_labels]\n",
    "    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, predictions = valid(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"@HuggingFace is a company based in New York, but is also has employees working in Paris\"\n",
    "model.eval()\n",
    "def inference(sentence):\n",
    "    inputs = tokenizer(sentence,\n",
    "#                         is_split_into_words=True, \n",
    "                        return_offsets_mapping=True, \n",
    "                        padding='max_length', \n",
    "                        truncation=True, \n",
    "                        max_length=config['max_length'],\n",
    "                        return_tensors=\"pt\")\n",
    "\n",
    "    # move to gpu\n",
    "    ids = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "    # forward pass\n",
    "    outputs = model(ids, attention_mask=mask, return_dict=False)\n",
    "    logits = outputs[0]\n",
    "\n",
    "    active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
    "\n",
    "    prediction = []\n",
    "    out_str = []\n",
    "    off_list = inputs[\"offset_mapping\"].squeeze().tolist()\n",
    "    for idx, mapping in enumerate(off_list):\n",
    "#         print(mapping, token_pred[1], token_pred[0],\"####\")\n",
    "\n",
    "#         only predictions on first word pieces are important\n",
    "        if mapping[0] != 0 and mapping[0] != off_list[idx-1][1]:\n",
    "#             print(mapping, token_pred[1], token_pred[0])\n",
    "            prediction.append(wp_preds[idx][1])\n",
    "            out_str.append(wp_preds[idx][0])\n",
    "        else:\n",
    "            if idx == 1:\n",
    "                prediction.append(wp_preds[idx][1])\n",
    "                out_str.append(wp_preds[idx][0])\n",
    "            continue\n",
    "    return prediction, out_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "\n",
    "for i, t in enumerate(test_texts['text'].tolist()):\n",
    "    o,o_t = inference(t)\n",
    "    y_pred.append(o)\n",
    "    l = train_text_df['entities'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = []\n",
    "import pdb\n",
    "for i in tqdm(range(len(test_texts))):\n",
    "#     pdb.set_trace()\n",
    "    idx = test_texts.id.values[i]\n",
    "#     pred = ['']*len(test_texts[i])\n",
    "\n",
    "#     for j in range(len(y_pred[i])):\n",
    "#         if words[i][j] != None:\n",
    "#             pred[words[i][j]] = labels[y_pred[i][j]]\n",
    "\n",
    "    pred = [x.replace('B-','').replace('I-','') for x in y_pred[i]]\n",
    "#     print(pred)\n",
    "    preds = []\n",
    "    j = 0\n",
    "    while j < len(pred):\n",
    "        cls = pred[j]\n",
    "#         pdb.set_trace()\n",
    "        if cls == 'O':\n",
    "            j += 1\n",
    "        end = j + 1\n",
    "        while end < len(pred) and pred[end] == cls:\n",
    "            end += 1\n",
    "            \n",
    "        if cls != 'O' and cls != '' and end - j > 10:\n",
    "            final_preds.append((idx, cls, ' '.join(map(str, list(range(j, end))))))\n",
    "        \n",
    "        j = end\n",
    "        \n",
    "print(final_preds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./sample_submission.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(final_preds)\n",
    "sub.columns = test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8344be4878adc6549e572326e5ceaef6fa6a6636d73e1c07dab1555c32c9e2c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
