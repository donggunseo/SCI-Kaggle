{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lead',\n",
       " 'Position',\n",
       " 'Evidence',\n",
       " 'Claim',\n",
       " 'Concluding Statement',\n",
       " 'Counterclaim',\n",
       " 'Rebuttal']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train.discourse_type.unique().tolist()\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "tags = defaultdict()\n",
    "\n",
    "for i, c in enumerate(classes):\n",
    "    tags[f'B-{c}'] = i\n",
    "    tags[f'I-{c}'] = i + len(classes)\n",
    "tags[f'O'] = len(classes) * 2\n",
    "tags[f'Special'] = -100\n",
    "    \n",
    "l2i = dict(tags)\n",
    "\n",
    "i2l = defaultdict()\n",
    "for k, v in l2i.items(): \n",
    "    i2l[v] = k\n",
    "i2l[-100] = 'Special'\n",
    "\n",
    "i2l = dict(i2l)\n",
    "\n",
    "N_LABELS = len(i2l) - 1 # not accounting for -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-Lead': 0,\n",
       " 'I-Lead': 7,\n",
       " 'B-Position': 1,\n",
       " 'I-Position': 8,\n",
       " 'B-Evidence': 2,\n",
       " 'I-Evidence': 9,\n",
       " 'B-Claim': 3,\n",
       " 'I-Claim': 10,\n",
       " 'B-Concluding Statement': 4,\n",
       " 'I-Concluding Statement': 11,\n",
       " 'B-Counterclaim': 5,\n",
       " 'I-Counterclaim': 12,\n",
       " 'B-Rebuttal': 6,\n",
       " 'I-Rebuttal': 13,\n",
       " 'O': 14,\n",
       " 'Special': -100}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15594/15594 [00:00<00:00, 36335.62it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4BCA0580352F</td>\n",
       "      <td>Isn't it wonderful to know that you get a chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29A8E6DA6F04</td>\n",
       "      <td>One uses a car to go to the store, pick someon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17E8BB6C550B</td>\n",
       "      <td>Driverless cars do seem like the thing today's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319B2511943D</td>\n",
       "      <td>Have you ever thought your choice wasn't the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C9B1827498E4</td>\n",
       "      <td>Imagine you're sitting at home, watching a sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text\n",
       "0  4BCA0580352F  Isn't it wonderful to know that you get a chan...\n",
       "1  29A8E6DA6F04  One uses a car to go to the store, pick someon...\n",
       "2  17E8BB6C550B  Driverless cars do seem like the thing today's...\n",
       "3  319B2511943D  Have you ever thought your choice wasn't the b...\n",
       "4  C9B1827498E4  Imagine you're sitting at home, watching a sho..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_names, train_texts = [], []\n",
    "for f in tqdm(list(os.listdir('../input/feedback-prize-2021/train'))):\n",
    "    test_names.append(f.replace('.txt', ''))\n",
    "    train_texts.append(open('../input/feedback-prize-2021/train/' + f, 'r', encoding='utf-8').read())\n",
    "train_text_df = pd.DataFrame({'id': test_names, 'text': train_texts})\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 , 100 , 200 , 300 , 400 , 500 , 600 , 700 , 800 , 900 , 1000 , 1100 , 1200 , 1300 , 1400 , 1500 , 1600 , 1700 , 1800 , 1900 , 2000 , 2100 , 2200 , 2300 , 2400 , 2500 , 2600 , 2700 , 2800 , 2900 , 3000 , 3100 , 3200 , 3300 , 3400 , 3500 , 3600 , 3700 , 3800 , 3900 , 4000 , 4100 , 4200 , 4300 , 4400 , 4500 , 4600 , 4700 , 4800 , 4900 , 5000 , 5100 , 5200 , 5300 , 5400 , 5500 , 5600 , 5700 , 5800 , 5900 , 6000 , 6100 , 6200 , 6300 , 6400 , 6500 , 6600 , 6700 , 6800 , 6900 , 7000 , 7100 , 7200 , 7300 , 7400 , 7500 , 7600 , 7700 , 7800 , 7900 , 8000 , 8100 , 8200 , 8300 , 8400 , 8500 , 8600 , 8700 , 8800 , 8900 , 9000 , 9100 , 9200 , 9300 , 9400 , 9500 , 9600 , 9700 , 9800 , 9900 , 10000 , 10100 , 10200 , 10300 , 10400 , 10500 , 10600 , 10700 , 10800 , 10900 , 11000 , 11100 , 11200 , 11300 , 11400 , 11500 , 11600 , 11700 , 11800 , 11900 , 12000 , 12100 , 12200 , 12300 , 12400 , 12500 , 12600 , 12700 , 12800 , 12900 , 13000 , 13100 , 13200 , 13300 , 13400 , 13500 , 13600 , 13700 , 13800 , 13900 , 14000 , 14100 , 14200 , 14300 , 14400 , 14500 , 14600 , 14700 , 14800 , 14900 , 15000 , 15100 , 15200 , 15300 , 15400 , 15500 , "
     ]
    }
   ],
   "source": [
    "all_entities = []\n",
    "all_text_list = []\n",
    "for ii,i in enumerate(train_text_df.iterrows()):\n",
    "    if ii%100==0: print(ii,', ',end='')\n",
    "    total = i[1]['text'].split().__len__()\n",
    "    text_list = i[1]['text'].split()\n",
    "    entities = [\"O\"]*total\n",
    "    for j in train[train['id'] == i[1]['id']].iterrows():\n",
    "        discourse = j[1]['discourse_type']\n",
    "        list_ix = [int(x) for x in j[1]['predictionstring'].split(' ')]\n",
    "        entities[list_ix[0]] = f\"B-{discourse}\"\n",
    "        for k in list_ix[1:]: entities[k] = f\"I-{discourse}\"\n",
    "    all_entities.append(entities)\n",
    "    all_text_list.append(text_list)\n",
    "train_text_df['entities'] = all_entities\n",
    "train_text_df['text_list'] = all_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15594, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>entities</th>\n",
       "      <th>text_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4BCA0580352F</td>\n",
       "      <td>Isn't it wonderful to know that you get a chan...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "      <td>[Isn't, it, wonderful, to, know, that, you, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29A8E6DA6F04</td>\n",
       "      <td>One uses a car to go to the store, pick someon...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "      <td>[One, uses, a, car, to, go, to, the, store,, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17E8BB6C550B</td>\n",
       "      <td>Driverless cars do seem like the thing today's...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "      <td>[Driverless, cars, do, seem, like, the, thing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>319B2511943D</td>\n",
       "      <td>Have you ever thought your choice wasn't the b...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "      <td>[Have, you, ever, thought, your, choice, wasn'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C9B1827498E4</td>\n",
       "      <td>Imagine you're sitting at home, watching a sho...</td>\n",
       "      <td>[B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...</td>\n",
       "      <td>[Imagine, you're, sitting, at, home,, watching...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  4BCA0580352F  Isn't it wonderful to know that you get a chan...   \n",
       "1  29A8E6DA6F04  One uses a car to go to the store, pick someon...   \n",
       "2  17E8BB6C550B  Driverless cars do seem like the thing today's...   \n",
       "3  319B2511943D  Have you ever thought your choice wasn't the b...   \n",
       "4  C9B1827498E4  Imagine you're sitting at home, watching a sho...   \n",
       "\n",
       "                                            entities  \\\n",
       "0  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...   \n",
       "1  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...   \n",
       "2  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...   \n",
       "3  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...   \n",
       "4  [B-Lead, I-Lead, I-Lead, I-Lead, I-Lead, I-Lea...   \n",
       "\n",
       "                                           text_list  \n",
       "0  [Isn't, it, wonderful, to, know, that, you, ge...  \n",
       "1  [One, uses, a, car, to, go, to, the, store,, p...  \n",
       "2  [Driverless, cars, do, seem, like, the, thing,...  \n",
       "3  [Have, you, ever, thought, your, choice, wasn'...  \n",
       "4  [Imagine, you're, sitting, at, home,, watching...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( train_text_df.shape)\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'text_list'],\n",
       "        num_rows: 14034\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'entities', 'text_list'],\n",
       "        num_rows: 1560\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds = Dataset.from_pandas(train_text_df)\n",
    "datasets = ds.train_test_split(test_size = 0.1, shuffle=True, seed=42)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8c0e24831543f7b1c48a5e444920b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1999eae99814faa822102c469eb065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0eb06e236dd46eb8e5ce8aefd0d9f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2479f95b7d2f46ceab1f08d8b077ad24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_beginnings(labels):\n",
    "    for i in range(1,len(labels)):\n",
    "        curr_lab = labels[i]\n",
    "        prev_lab = labels[i-1]\n",
    "        if curr_lab in range(7,14):\n",
    "            if prev_lab != curr_lab and prev_lab != curr_lab - 7:\n",
    "                labels[i] = curr_lab -7\n",
    "    return labels\n",
    "\n",
    "def preparing_train_dataset(examples):\n",
    "    encoding = tokenizer(examples['text_list'], truncation=True, padding=True, max_length = 1024, is_split_into_words=True)\n",
    "    total= len(encoding['input_ids'])\n",
    "    encoding['labels']=[]\n",
    "    for i in range(total):\n",
    "        labels = [l2i['O'] for _ in range(len(encoding['input_ids'][i]))]\n",
    "        word_idx = encoding.word_ids(batch_index=i)\n",
    "        for j in range(len(word_idx)):\n",
    "            if word_idx[j] is None:\n",
    "                labels[j]=l2i['Special']\n",
    "            else:\n",
    "                label = examples['entities'][i][word_idx[j]]\n",
    "                if label[0]=='B' and j!=0 and word_idx[j-1]==word_idx[j]:\n",
    "                    label_t = label.replace('B', 'I')\n",
    "                    labels[j]=l2i[label_t]\n",
    "                else:\n",
    "                    labels[j]=l2i[label]\n",
    "        labels = fix_beginnings(labels)\n",
    "        encoding['labels'].append(labels)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165c2f36aa37439aba66139c6d53f9c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ec282948ab4d548b2df8b654a7a792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(preparing_train_dataset, batched=True, batch_size=10000, remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c60dfef2a64d9ab199cfda2dc40191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForTokenClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerForTokenClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained('allenai/longformer-base-4096')\n",
    "config.num_labels = N_LABELS\n",
    "model = AutoModelForTokenClassification.from_pretrained('allenai/longformer-base-4096', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = './output/longformer-baseline',\n",
    "    evaluation_strategy = 'steps',\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay = 0.01,\n",
    "    max_grad_norm = 10,\n",
    "    num_train_epochs = 5,\n",
    "    warmup_ratio = 0.1,\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = 10,\n",
    "    save_strategy = 'steps',\n",
    "    save_total_limit = 1,\n",
    "    seed = 42,\n",
    "    eval_steps = 50,\n",
    "    save_steps = 50,\n",
    "    dataloader_num_workers = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'f1',# need to fix\n",
    "    group_by_length = True,\n",
    "    report_to = 'wandb',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric('seqeval')\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = [\n",
    "        [i2l[p] for (p, l) in zip(prediction, label) if l!= -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [i2l[l] for (p, l) in zip(prediction, label) if l!= -100] for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uv46k7hl) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 25686... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "739fd3195bec4876b620a20e944521be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">longformer-baseline</strong>: <a href=\"https://wandb.ai/donggunseo/Feedback-prize/runs/uv46k7hl\" target=\"_blank\">https://wandb.ai/donggunseo/Feedback-prize/runs/uv46k7hl</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220126_071420-uv46k7hl/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uv46k7hl). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/donggunseo/Feedback-prize/runs/1fwcpvzs\" target=\"_blank\">longformer-baseline</a></strong> to <a href=\"https://wandb.ai/donggunseo/Feedback-prize\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 14034\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 1460\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1460' max='1460' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1460/1460 1:18:02, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.704700</td>\n",
       "      <td>1.612643</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>0.540288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>1.431925</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.540638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.077600</td>\n",
       "      <td>0.971407</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>0.015795</td>\n",
       "      <td>0.011750</td>\n",
       "      <td>0.706571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>0.868745</td>\n",
       "      <td>0.009739</td>\n",
       "      <td>0.020159</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>0.731665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.906300</td>\n",
       "      <td>0.793720</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.047246</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.749078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.862700</td>\n",
       "      <td>0.760780</td>\n",
       "      <td>0.033579</td>\n",
       "      <td>0.071216</td>\n",
       "      <td>0.045639</td>\n",
       "      <td>0.758025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.787000</td>\n",
       "      <td>0.730735</td>\n",
       "      <td>0.042784</td>\n",
       "      <td>0.078836</td>\n",
       "      <td>0.055466</td>\n",
       "      <td>0.765646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.752200</td>\n",
       "      <td>0.716392</td>\n",
       "      <td>0.057560</td>\n",
       "      <td>0.110980</td>\n",
       "      <td>0.075804</td>\n",
       "      <td>0.769912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.740500</td>\n",
       "      <td>0.688651</td>\n",
       "      <td>0.072217</td>\n",
       "      <td>0.141046</td>\n",
       "      <td>0.095524</td>\n",
       "      <td>0.775784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.743500</td>\n",
       "      <td>0.678463</td>\n",
       "      <td>0.078607</td>\n",
       "      <td>0.161967</td>\n",
       "      <td>0.105845</td>\n",
       "      <td>0.777663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.759600</td>\n",
       "      <td>0.665866</td>\n",
       "      <td>0.087510</td>\n",
       "      <td>0.176585</td>\n",
       "      <td>0.117026</td>\n",
       "      <td>0.780969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.710900</td>\n",
       "      <td>0.681623</td>\n",
       "      <td>0.096214</td>\n",
       "      <td>0.201732</td>\n",
       "      <td>0.130288</td>\n",
       "      <td>0.775537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.675200</td>\n",
       "      <td>0.656337</td>\n",
       "      <td>0.118036</td>\n",
       "      <td>0.231451</td>\n",
       "      <td>0.156341</td>\n",
       "      <td>0.785618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.656188</td>\n",
       "      <td>0.114962</td>\n",
       "      <td>0.235746</td>\n",
       "      <td>0.154555</td>\n",
       "      <td>0.783138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.679600</td>\n",
       "      <td>0.654315</td>\n",
       "      <td>0.117588</td>\n",
       "      <td>0.209214</td>\n",
       "      <td>0.150556</td>\n",
       "      <td>0.785746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.653433</td>\n",
       "      <td>0.110514</td>\n",
       "      <td>0.245168</td>\n",
       "      <td>0.152353</td>\n",
       "      <td>0.780669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.681800</td>\n",
       "      <td>0.644373</td>\n",
       "      <td>0.124948</td>\n",
       "      <td>0.249671</td>\n",
       "      <td>0.166547</td>\n",
       "      <td>0.786288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.657100</td>\n",
       "      <td>0.638732</td>\n",
       "      <td>0.126801</td>\n",
       "      <td>0.250572</td>\n",
       "      <td>0.168389</td>\n",
       "      <td>0.789547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.698900</td>\n",
       "      <td>0.641368</td>\n",
       "      <td>0.125612</td>\n",
       "      <td>0.261240</td>\n",
       "      <td>0.169651</td>\n",
       "      <td>0.785451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.672800</td>\n",
       "      <td>0.631453</td>\n",
       "      <td>0.133279</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.176062</td>\n",
       "      <td>0.790036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.633400</td>\n",
       "      <td>0.637777</td>\n",
       "      <td>0.128026</td>\n",
       "      <td>0.275165</td>\n",
       "      <td>0.174747</td>\n",
       "      <td>0.783832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.648400</td>\n",
       "      <td>0.628264</td>\n",
       "      <td>0.128786</td>\n",
       "      <td>0.257430</td>\n",
       "      <td>0.171683</td>\n",
       "      <td>0.791607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>0.631594</td>\n",
       "      <td>0.129103</td>\n",
       "      <td>0.279044</td>\n",
       "      <td>0.176531</td>\n",
       "      <td>0.786372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.647400</td>\n",
       "      <td>0.626319</td>\n",
       "      <td>0.139179</td>\n",
       "      <td>0.277520</td>\n",
       "      <td>0.185386</td>\n",
       "      <td>0.791574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.624991</td>\n",
       "      <td>0.137868</td>\n",
       "      <td>0.276481</td>\n",
       "      <td>0.183989</td>\n",
       "      <td>0.790516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.647500</td>\n",
       "      <td>0.625254</td>\n",
       "      <td>0.139624</td>\n",
       "      <td>0.276134</td>\n",
       "      <td>0.185469</td>\n",
       "      <td>0.790890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.651100</td>\n",
       "      <td>0.622762</td>\n",
       "      <td>0.138863</td>\n",
       "      <td>0.274195</td>\n",
       "      <td>0.184359</td>\n",
       "      <td>0.792318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.632400</td>\n",
       "      <td>0.625608</td>\n",
       "      <td>0.139385</td>\n",
       "      <td>0.281676</td>\n",
       "      <td>0.186488</td>\n",
       "      <td>0.790375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.623173</td>\n",
       "      <td>0.142580</td>\n",
       "      <td>0.280014</td>\n",
       "      <td>0.188949</td>\n",
       "      <td>0.792148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-50\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-50/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-50/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-100\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-100/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-100/special_tokens_map.json\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-150\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-150/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-150/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-50] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-200\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-200/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-200/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "/usr/local/lib/python3.6/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-250\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-250/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-150] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-300\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-300/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-300/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-350\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-350/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-350/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-400\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-400/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-450\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-450/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-450/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-350] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-500\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-500/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-400] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-550\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-550/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-550/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-550/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-550/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-450] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-600\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-600/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-600/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-500] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-650\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-650/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-650/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-650/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-650/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-550] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-700\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-700/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-700/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-600] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-750\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-750/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-750/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-700] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-800\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-800/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-800/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-750] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-850\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-850/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-850/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-850/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-850/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-650] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-900\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-900/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-900/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-800] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-950\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-950/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-950/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-950/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-950/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-850] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1000\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1000/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-900] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1050\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1050/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1050/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1050/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-950] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1100\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1100/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1100/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1100/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1100/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1050] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1150\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1150/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1150/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1150/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1150/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1200\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1200/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1200/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1200/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1100] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1250\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1250/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1250/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1250/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1250/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1150] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1300\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1300/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1300/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1300/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1300/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1200] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1350\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1350/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1350/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1350/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1350/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1250] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1400\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1400/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1400/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1400/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1400/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1300] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1560\n",
      "  Batch size = 24\n",
      "Saving model checkpoint to ./output/longformer-baseline/checkpoint-1450\n",
      "Configuration saved in ./output/longformer-baseline/checkpoint-1450/config.json\n",
      "Model weights saved in ./output/longformer-baseline/checkpoint-1450/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/longformer-baseline/checkpoint-1450/tokenizer_config.json\n",
      "Special tokens file saved in ./output/longformer-baseline/checkpoint-1450/special_tokens_map.json\n",
      "Deleting older checkpoint [output/longformer-baseline/checkpoint-1350] due to args.save_total_limit\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/longformer-baseline/checkpoint-1450 (score: 0.18894913986537024).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 26821... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f98026aed4649378eecd37b3206166d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▁▆▆▇▇▇▇█████████████████████</td></tr><tr><td>eval/f1</td><td>▁▁▁▁▂▃▃▄▅▅▅▆▇▇▇▇▇▇▇█▇▇███████</td></tr><tr><td>eval/loss</td><td>█▇▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/precision</td><td>▁▁▁▁▂▃▃▄▅▅▅▆▇▇▇▆▇▇▇█▇▇▇██████</td></tr><tr><td>eval/recall</td><td>▁▁▁▁▂▃▃▄▅▅▅▆▇▇▆▇▇▇▇▇█▇███████</td></tr><tr><td>eval/runtime</td><td>▁▁▂▄▅▃█▆▁▄▅▄▅▃▅▃▂▂▅▆▄▃▄▂▄▁▄▄▄</td></tr><tr><td>eval/samples_per_second</td><td>██▇▅▄▆▁▃█▅▄▅▄▆▄▆▇▇▄▃▅▆▅▆▅█▅▅▅</td></tr><tr><td>eval/steps_per_second</td><td>██▇▅▄▆▁▃█▅▄▅▄▆▄▆▇▇▄▃▅▆▅▆▅█▅▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.79215</td></tr><tr><td>eval/f1</td><td>0.18895</td></tr><tr><td>eval/loss</td><td>0.62317</td></tr><tr><td>eval/precision</td><td>0.14258</td></tr><tr><td>eval/recall</td><td>0.28001</td></tr><tr><td>eval/runtime</td><td>38.9918</td></tr><tr><td>eval/samples_per_second</td><td>40.008</td></tr><tr><td>eval/steps_per_second</td><td>1.667</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>1460</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.618</td></tr><tr><td>train/total_flos</td><td>4.582920975202714e+16</td></tr><tr><td>train/train_loss</td><td>0.79952</td></tr><tr><td>train/train_runtime</td><td>4684.7157</td></tr><tr><td>train/train_samples_per_second</td><td>14.978</td></tr><tr><td>train/train_steps_per_second</td><td>0.312</td></tr></table>\n",
       "</div></div>\n",
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">longformer-baseline</strong>: <a href=\"https://wandb.ai/donggunseo/Feedback-prize/runs/1fwcpvzs\" target=\"_blank\">https://wandb.ai/donggunseo/Feedback-prize/runs/1fwcpvzs</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220126_071558-1fwcpvzs/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to longformer-baseline_best\n",
      "Configuration saved in longformer-baseline_best/config.json\n",
      "Model weights saved in longformer-baseline_best/pytorch_model.bin\n",
      "tokenizer config file saved in longformer-baseline_best/tokenizer_config.json\n",
      "Special tokens file saved in longformer-baseline_best/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "run = wandb.init(project='Feedback-prize', entity='donggunseo', name='longformer-baseline')\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,2\"\n",
    "trainer.train()\n",
    "run.finish()\n",
    "trainer.save_model('longformer-baseline_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8344be4878adc6549e572326e5ceaef6fa6a6636d73e1c07dab1555c32c9e2c5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
